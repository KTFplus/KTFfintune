{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1a15fe-6935-462e-af28-50af5f652b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 00:46:57,853 - INFO - Using GPU: NVIDIA GeForce RTX 3090\n",
      "2025-06-14 00:46:57,854 - INFO - GPU Memory: 25.4 GB\n",
      "2025-06-14 00:46:57,854 - INFO - Starting Whisper Korean fine-tuning...\n",
      "2025-06-14 00:46:59,073 - INFO - Loading existing metadata files...\n",
      "2025-06-14 00:47:01,055 - INFO - model, processor loaded... \n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "2025-06-14 00:47:01,364 - INFO - Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 4:12:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 00:55:58,095 - INFO - Step 100: Loss = 2.1648\n",
      "2025-06-14 01:05:36,697 - INFO - Step 200: Loss = 0.7581\n",
      "2025-06-14 01:15:28,264 - INFO - Step 300: Loss = 0.5202\n",
      "2025-06-14 01:25:29,612 - INFO - Step 400: Loss = 0.2130\n",
      "2025-06-14 01:35:18,793 - INFO - Step 500: Loss = 0.1620\n",
      "2025-06-14 01:45:42,893 - INFO - Step 600: Loss = 0.1300\n",
      "2025-06-14 01:56:33,585 - INFO - Step 700: Loss = 0.1096\n",
      "2025-06-14 02:07:16,559 - INFO - Step 800: Loss = 0.0970\n",
      "2025-06-14 02:17:44,410 - INFO - Step 900: Loss = 0.0811\n",
      "2025-06-14 02:27:54,074 - INFO - Step 1000: Loss = 0.0735\n",
      "2025-06-14 02:37:57,285 - INFO - Step 1100: Loss = 0.0677\n",
      "2025-06-14 02:48:01,930 - INFO - Step 1200: Loss = 0.0545\n",
      "2025-06-14 02:58:25,987 - INFO - Step 1300: Loss = 0.0309\n",
      "2025-06-14 03:08:51,549 - INFO - Step 1400: Loss = 0.0323\n",
      "2025-06-14 03:19:23,027 - INFO - Step 1500: Loss = 0.0270\n",
      "2025-06-14 03:29:08,600 - INFO - Step 1600: Loss = 0.0294\n",
      "2025-06-14 03:38:53,034 - INFO - Step 1700: Loss = 0.0273\n",
      "2025-06-14 03:48:35,154 - INFO - Step 1800: Loss = 0.0235\n",
      "2025-06-14 03:58:20,561 - INFO - Step 1900: Loss = 0.0209\n",
      "2025-06-14 04:08:44,924 - INFO - Step 2000: Loss = 0.0235\n",
      "2025-06-14 04:19:09,135 - INFO - Step 2100: Loss = 0.0208\n",
      "2025-06-14 04:29:28,165 - INFO - Step 2200: Loss = 0.0224\n",
      "2025-06-14 04:39:39,102 - INFO - Step 2300: Loss = 0.0192\n",
      "2025-06-14 04:49:27,205 - INFO - Step 2400: Loss = 0.0098\n",
      "2025-06-14 04:59:12,710 - INFO - Step 2500: Loss = 0.0092\n",
      "2025-06-14 04:59:17,508 - INFO - Training logs saved to whisper-small-korean-finetuned/training_logs.json\n",
      "2025-06-14 04:59:17,509 - INFO - Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='1032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 295/1032 01:32 < 03:51, 3.19 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 05:00:50,322 - ERROR - Training failed: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 387\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU not available, using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m \u001b[43mtrain_whisper_korean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 362\u001b[0m, in \u001b[0;36mtrain_whisper_korean\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_log_history(model_output_dir)\n\u001b[1;32m    361\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning final evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 362\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal evaluation results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3975\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3972\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3974\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3975\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3985\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:4196\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4194\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4196\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4198\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:94\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     91\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "from jiwer import wer, cer\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('whisper_finetuning.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class KoreanWhisperDataPreprocessor:\n",
    "    def __init__(self, audio_dir: str, csv_files: List[str], output_dir: str, feature_dir: str, chunk_size: int = 1000):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.csv_files = csv_files\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.feature_dir = Path(feature_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.feature_dir.mkdir(exist_ok=True)\n",
    "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        text = re.sub(r'[\",\\'\\\"]', '', text)\n",
    "        text = re.sub(r'[^\\w\\s.!?ㄱ-ㅎㅏ-ㅣ가-힣]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_and_merge_data(self) -> pd.DataFrame:\n",
    "        dataframes = []\n",
    "        for csv_file in self.csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dataframes.append(df)\n",
    "            logger.info(f\"Loaded {len(df)} samples from {csv_file}\")\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        logger.info(f\"Total samples after merging: {len(merged_df)}\")\n",
    "        return merged_df\n",
    "\n",
    "    def validate_audio_files(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        valid_indices = []\n",
    "        for idx, row in df.iterrows():\n",
    "            audio_path = self.audio_dir / row['fileName']\n",
    "            if audio_path.exists():\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                logger.warning(f\"Audio file not found: {audio_path}\")\n",
    "        valid_df = df.loc[valid_indices].reset_index(drop=True)\n",
    "        logger.info(f\"Valid audio files: {len(valid_df)}/{len(df)}\")\n",
    "        return valid_df\n",
    "\n",
    "    def process_and_save_feature(self, audio_path: Path, feature_path: Path) -> bool:\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                waveform = resampler(waveform)\n",
    "            audio_array = waveform.squeeze().numpy().astype(np.float32)\n",
    "            max_length = 30 * 16000\n",
    "            if len(audio_array) > max_length:\n",
    "                audio_array = audio_array[:max_length]\n",
    "                logger.warning(f\"Audio truncated: {audio_path}\")\n",
    "            # (1, 80, N) → (80, N)\n",
    "            features = self.processor.feature_extractor(audio_array, sampling_rate=16000).input_features.squeeze(0)\n",
    "            np.save(feature_path, features)\n",
    "            del waveform, audio_array, features\n",
    "            gc.collect()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing audio {audio_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def create_dataset(self) -> (HFDataset, HFDataset):\n",
    "        train_meta_path = self.output_dir / \"train_metadata.json\"\n",
    "        val_meta_path = self.output_dir / \"val_metadata.json\"\n",
    "\n",
    "        if train_meta_path.exists() and val_meta_path.exists():\n",
    "            logger.info(\"Loading existing metadata files...\")\n",
    "            with open(train_meta_path, 'r', encoding='utf-8') as f:\n",
    "                train_meta = json.load(f)\n",
    "            with open(val_meta_path, 'r', encoding='utf-8') as f:\n",
    "                val_meta = json.load(f)\n",
    "        else:\n",
    "            logger.info(\"Creating new metadata files and extracting features...\")\n",
    "            df = self.load_and_merge_data()\n",
    "            df = self.validate_audio_files(df)\n",
    "            df['cleaned_text'] = df['Reading'].apply(self.clean_text)\n",
    "            df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "            train_meta = []\n",
    "            val_meta = []\n",
    "            meta_chunks = []\n",
    "            for idx, row in df.iterrows():\n",
    "                if idx % self.chunk_size == 0 and idx > 0:\n",
    "                    logger.info(f\"Processed {idx}/{len(df)} samples, writing chunk to disk and clearing memory...\")\n",
    "                    meta_chunks.append((train_meta.copy(), val_meta.copy()))\n",
    "                    train_meta.clear()\n",
    "                    val_meta.clear()\n",
    "                    gc.collect()\n",
    "                audio_path = self.audio_dir / row['fileName']\n",
    "                feature_path = self.feature_dir / (row['fileName'] + \".npy\")\n",
    "                if not feature_path.exists():\n",
    "                    success = self.process_and_save_feature(audio_path, feature_path)\n",
    "                    if not success:\n",
    "                        continue\n",
    "                meta = {\n",
    "                    \"feature_path\": str(feature_path),\n",
    "                    \"text\": row['cleaned_text'],\n",
    "                    \"file_name\": row['fileName'],\n",
    "                    \"duration\": row['recordTime']\n",
    "                }\n",
    "                if idx < int(len(df) * 0.8):\n",
    "                    train_meta.append(meta)\n",
    "                else:\n",
    "                    val_meta.append(meta)\n",
    "            meta_chunks.append((train_meta, val_meta))\n",
    "            all_train_meta = []\n",
    "            all_val_meta = []\n",
    "            for t, v in meta_chunks:\n",
    "                all_train_meta.extend(t)\n",
    "                all_val_meta.extend(v)\n",
    "            with open(train_meta_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_train_meta, f, ensure_ascii=False, indent=2)\n",
    "            with open(val_meta_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_val_meta, f, ensure_ascii=False, indent=2)\n",
    "            logger.info(f\"Saved metadata - Train: {len(all_train_meta)}, Val: {len(all_val_meta)}\")\n",
    "            train_meta = all_train_meta\n",
    "            val_meta = all_val_meta\n",
    "\n",
    "        train_dataset = HFDataset.from_list(train_meta)\n",
    "        val_dataset = HFDataset.from_list(val_meta)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "class WhisperDataset(Dataset):\n",
    "    def __init__(self, dataset: HFDataset, processor: WhisperProcessor, max_length: int = 448):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        features = np.load(item[\"feature_path\"])  # (80, N)\n",
    "        assert features.shape[0] == 80, f\"Invalid feature shape: {features.shape}\"\n",
    "        features = torch.tensor(features, dtype=torch.float)  # [80, N]\n",
    "        labels = self.processor.tokenizer(\n",
    "            item[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        ).input_ids.squeeze()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_features\": features,  # [80, N]\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "def whisper_collate_fn(batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 입력 패딩 (CPU에서 생성)\n",
    "    max_len = max(f[\"input_features\"].shape[1] for f in batch)\n",
    "    batch_size = len(batch)\n",
    "    input_tensor = torch.zeros((batch_size, 80, max_len), dtype=torch.float32)  # device 지정 X → CPU\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        feat = item[\"input_features\"].cpu()  # CPU로 이동\n",
    "        input_tensor[i, :, :feat.shape[1]] = feat\n",
    "    \n",
    "    # 라벨 패딩 (CPU에서 생성)\n",
    "    max_label_len = max(item[\"labels\"].size(0) for item in batch)\n",
    "    label_tensor = torch.full((batch_size, max_label_len), -100, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        l = item[\"labels\"].cpu()  # CPU로 이동\n",
    "        label_tensor[i, :l.size(0)] = l\n",
    "    \n",
    "    return {\n",
    "        \"input_features\": input_tensor.to('cuda'),  # CPU 텐서\n",
    "        \"labels\": label_tensor.to('cuda')\n",
    "    }\n",
    "\n",
    "\n",
    "class WhisperMetrics:\n",
    "    def __init__(self, processor: WhisperProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.where(predictions != -100, predictions, self.processor.tokenizer.pad_token_id)\n",
    "        decoded_preds = self.processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, self.processor.tokenizer.pad_token_id)\n",
    "        decoded_labels = self.processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        wer_score = wer(decoded_labels, decoded_preds)\n",
    "        cer_score = cer(decoded_labels, decoded_preds)\n",
    "        return {\n",
    "            \"wer\": wer_score,\n",
    "            \"cer\": cer_score\n",
    "        }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.log_history = []\n",
    "\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        super().log(logs)\n",
    "        self.log_history.append({\n",
    "            \"step\": self.state.global_step,\n",
    "            \"epoch\": self.state.epoch,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            **logs\n",
    "        })\n",
    "        if \"loss\" in logs:\n",
    "            logger.info(f\"Step {self.state.global_step}: Loss = {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            logger.info(f\"Validation - Loss: {logs['eval_loss']:.4f}, \"\n",
    "                        f\"WER: {logs.get('eval_wer', 0):.4f}, \"\n",
    "                        f\"CER: {logs.get('eval_cer', 0):.4f}\")\n",
    "\n",
    "    def save_log_history(self, output_dir: str):\n",
    "        log_path = Path(output_dir) / \"training_logs.json\"\n",
    "        with open(log_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.log_history, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"Training logs saved to {log_path}\")\n",
    "\n",
    "def train_whisper_korean():\n",
    "    audio_dir = \"train\"\n",
    "    csv_files = [\"filtered_data_A.csv\", \"filtered_data_B.csv\"]\n",
    "    output_dir = \"preprocessed_whisper\"\n",
    "    feature_dir = \"preprocessed_whisper/features\"\n",
    "    model_output_dir = \"whisper-small-korean-finetuned\"\n",
    "    logger.info(\"Starting Whisper Korean fine-tuning...\")\n",
    "\n",
    "    preprocessor = KoreanWhisperDataPreprocessor(audio_dir, csv_files, output_dir, feature_dir, chunk_size=500)\n",
    "    train_dataset, val_dataset = preprocessor.create_dataset()\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    model.config._attn_implementation = \"eager\"\n",
    "\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "\n",
    "    train_torch_dataset = WhisperDataset(train_dataset, processor)\n",
    "    val_torch_dataset = WhisperDataset(val_dataset, processor)\n",
    "\n",
    "    metrics = WhisperMetrics(processor)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=5000,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=False,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_cer\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        remove_unused_columns=False, \n",
    "        dataloader_pin_memory=False, \n",
    "        dataloader_num_workers=0,\n",
    "        report_to=None,\n",
    "        run_name=f\"whisper-korean-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_torch_dataset,\n",
    "        eval_dataset=val_torch_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        compute_metrics=metrics.compute_metrics,\n",
    "        data_collator=whisper_collate_fn,  # 중요!\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if os.path.exists(model_output_dir):\n",
    "        checkpoints = [d for d in os.listdir(model_output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_dir = os.path.join(model_output_dir, latest_checkpoint)\n",
    "            logger.info(f\"Resuming from checkpoint: {checkpoint_dir}\")\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_dir)\n",
    "        trainer.save_model()\n",
    "        processor.save_pretrained(model_output_dir)\n",
    "        trainer.save_log_history(model_output_dir)\n",
    "        logger.info(\"Running final evaluation...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        logger.info(f\"Final evaluation results: {eval_results}\")\n",
    "        config_for_upload = {\n",
    "            \"model_type\": \"whisper\",\n",
    "            \"task\": \"automatic-speech-recognition\",\n",
    "            \"language\": \"korean\",\n",
    "            \"dataset_size\": len(train_dataset) + len(val_dataset),\n",
    "            \"training_steps\": training_args.max_steps,\n",
    "            \"final_cer\": eval_results.get(\"eval_cer\", 0),\n",
    "            \"final_wer\": eval_results.get(\"eval_wer\", 0)\n",
    "        }\n",
    "        with open(f\"{model_output_dir}/training_info.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_for_upload, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"Model ready for HuggingFace upload at: {model_output_dir}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 멀티프로세싱 시작 방식 설정 (CUDA 사용 시 필수)\n",
    "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "        logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        logger.warning(\"GPU not available, using CPU\")\n",
    "    train_whisper_korean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69555316-b05c-4357-8d99-47eea687a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from jiwer import wer, cer\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from google.colab import files\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WhisperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor, max_length=448):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        features = np.load(item[\"feature_path\"])\n",
    "        assert features.shape[0] == 80, f\"Invalid feature shape: {features.shape}\"\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "        labels = self.processor.tokenizer(\n",
    "            item[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        ).input_ids.squeeze()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_features\": features,\n",
    "            \"labels\": labels,\n",
    "            \"text\": item[\"text\"],\n",
    "            \"file_name\": item[\"file_name\"]\n",
    "        }\n",
    "\n",
    "def evaluate_single_model(model_name, val_metadata_path, features_dir, num_samples=5, batch_size=4):\n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # GPU 메모리 확인\n",
    "    if device == \"cuda\":\n",
    "        logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        logger.info(f\"Available GPU Memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "    # Validation 데이터 로드\n",
    "    logger.info(\"Loading validation metadata...\")\n",
    "    with open(val_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        val_metadata = json.load(f)\n",
    "\n",
    "    # feature_path 수정\n",
    "    for item in val_metadata:\n",
    "        # 파일 이름만 추출\n",
    "        file_name = item[\"feature_path\"].split(\"/\")[-1]\n",
    "        # 새로운 경로로 수정\n",
    "        item[\"feature_path\"] = f\"/content/drive/MyDrive/preprocessed_whisper/features/{file_name}\"\n",
    "\n",
    "    val_dataset = Dataset.from_list(val_metadata)\n",
    "    logger.info(f\"Loaded {len(val_dataset)} validation samples\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 모델과 프로세서 로드\n",
    "    logger.info(f\"Loading model and processor: {model_name}\")\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "\n",
    "    # 데이터셋 준비\n",
    "    val_torch_dataset = WhisperDataset(val_dataset, processor)\n",
    "\n",
    "    # 평가 결과 저장\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    sample_results = []\n",
    "\n",
    "    # 배치 단위로 평가 진행\n",
    "    logger.info(\"Starting evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(0, len(val_torch_dataset), batch_size),\n",
    "                   desc=f\"Evaluating {model_name}\",\n",
    "                   ncols=100)\n",
    "\n",
    "        for idx in pbar:\n",
    "            batch_items = [val_torch_dataset[i] for i in range(idx, min(idx + batch_size, len(val_torch_dataset)))]\n",
    "\n",
    "            # 배치 데이터 준비\n",
    "            input_features = torch.stack([item[\"input_features\"] for item in batch_items]).to(device)\n",
    "\n",
    "            # 예측\n",
    "            predicted_ids = model.generate(input_features)\n",
    "            transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "            # 결과 저장\n",
    "            for i, (transcription, item) in enumerate(zip(transcriptions, batch_items)):\n",
    "                all_predictions.append(transcription)\n",
    "                all_references.append(item[\"text\"])\n",
    "\n",
    "                if idx + i < num_samples:\n",
    "                    sample_results.append({\n",
    "                        \"file_name\": item[\"file_name\"],\n",
    "                        \"reference\": item[\"text\"],\n",
    "                        \"prediction\": transcription\n",
    "                    })\n",
    "\n",
    "            # 진행 상황 업데이트\n",
    "            pbar.set_postfix({\n",
    "                'processed': f\"{min(idx + batch_size, len(val_torch_dataset))}/{len(val_torch_dataset)}\",\n",
    "                'memory': f\"{torch.cuda.memory_allocated() / 1e9:.1f}GB\"\n",
    "            })\n",
    "\n",
    "            # 메모리 정리\n",
    "            del input_features, predicted_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # 메트릭 계산\n",
    "    logger.info(\"Calculating metrics...\")\n",
    "    wer_score = wer(all_references, all_predictions)\n",
    "    cer_score = cer(all_references, all_predictions)\n",
    "\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"wer\": wer_score,\n",
    "        \"cer\": cer_score,\n",
    "        \"samples\": sample_results,\n",
    "        \"duration\": time.time() - start_time\n",
    "    }\n",
    "\n",
    "    logger.info(f\"\\nEvaluation completed in {results['duration']:.2f} seconds\")\n",
    "    logger.info(f\"WER: {wer_score:.4f}\")\n",
    "    logger.info(f\"CER: {cer_score:.4f}\")\n",
    "\n",
    "    # 샘플 결과 출력\n",
    "    logger.info(\"\\nSample Results:\")\n",
    "    for i, sample in enumerate(sample_results, 1):\n",
    "        logger.info(f\"\\nSample {i}:\")\n",
    "        logger.info(f\"File: {sample['file_name']}\")\n",
    "        logger.info(f\"Reference: {sample['reference']}\")\n",
    "        logger.info(f\"Prediction: {sample['prediction']}\")\n",
    "\n",
    "    # 메모리 정리\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_results(results_dir):\n",
    "    \"\"\"여러 모델의 평가 결과를 비교\"\"\"\n",
    "    all_results = {}\n",
    "    for result_file in os.listdir(results_dir):\n",
    "        if result_file.endswith('_results.json'):\n",
    "            with open(os.path.join(results_dir, result_file), 'r', encoding='utf-8') as f:\n",
    "                results = json.load(f)\n",
    "                model_name = results['model_name']\n",
    "                all_results[model_name] = {\n",
    "                    'wer': results['wer'],\n",
    "                    'cer': results['cer'],\n",
    "                    'duration': results['duration']\n",
    "                }\n",
    "\n",
    "    # 결과 비교 출력\n",
    "    logger.info(\"\\nModel Comparison Summary:\")\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"{'Model Name':<30} {'WER':<10} {'CER':<10} {'Duration (s)':<15}\")\n",
    "    logger.info(\"-\"*70)\n",
    "    for model_name, metrics in all_results.items():\n",
    "        logger.info(f\"{model_name:<30} {metrics['wer']:<10.4f} {metrics['cer']:<10.4f} {metrics['duration']:<15.2f}\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "def main():\n",
    "\n",
    "    # GPU 메모리 최적화\n",
    "    logger.info(\"Optimizing GPU memory...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # 평가할 모델들\n",
    "    model_names = [\n",
    "        \"urewui/ktf\",\n",
    "        \"openai/whisper-small\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 결과 저장 디렉토리 생성\n",
    "    results_dir = \"evaluation_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # 각 모델별로 개별 평가\n",
    "    for model_name in model_names:\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Starting evaluation for {model_name}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "\n",
    "        # 모델 평가\n",
    "        results = evaluate_single_model(\n",
    "            model_name=model_name,\n",
    "            val_metadata_path=\"val_metadata.json\",\n",
    "            features_dir=\"preprocessed_whisper/features\",\n",
    "            num_samples=5,\n",
    "            batch_size=2\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        result_file = os.path.join(results_dir, f\"{model_name.replace('/', '_')}_results.json\")\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 결과 파일 다운로드\n",
    "        files.download(result_file)\n",
    "\n",
    "    # 모든 결과 비교\n",
    "    compare_results(results_dir)\n",
    "\n",
    "    logger.info(\"All evaluations completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabf171-9edb-4676-9b7b-a2ee617ac49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import json\n",
    "\n",
    "# val 데이터셋 메타데이터 경로\n",
    "val_dataset_path = \"preprocessed_whisper/val_metadata.json\"\n",
    "\n",
    "# 체크포인트 디렉토리 및 리스트\n",
    "checkpoint_dir = \"whisper-small-korean-finetuned\"\n",
    "checkpoints = [\"checkpoint-500\", \"checkpoint-1000\", \"checkpoint-1500\"]\n",
    "\n",
    "# val 메타데이터 로드\n",
    "with open(val_dataset_path, 'r', encoding='utf-8') as f:\n",
    "    val_meta = json.load(f)\n",
    "\n",
    "# WhisperProcessor 로드\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint_dir)\n",
    "\n",
    "def transcribe_checkpoint(checkpoint_path, val_meta, processor, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", max_samples=None):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, item in enumerate(val_meta):\n",
    "        if max_samples is not None and i >= max_samples:\n",
    "            break\n",
    "        feature_path = item[\"feature_path\"]\n",
    "        text = item[\"text\"]\n",
    "        file_name = item[\"file_name\"]\n",
    "\n",
    "        features = np.load(feature_path)  # (80, N)\n",
    "        features = torch.tensor(features, dtype=torch.float).unsqueeze(0).to(device)  # [1, 80, N]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(features)\n",
    "        transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        transcriptions.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"reference\": text,\n",
    "            \"transcription\": transcription\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1} samples...\")\n",
    "\n",
    "    return transcriptions\n",
    "\n",
    "all_transcriptions = {}\n",
    "for ckpt in checkpoints:\n",
    "    ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(f\"\\nTranscribing with {ckpt} ...\")\n",
    "        transcriptions = transcribe_checkpoint(ckpt_path, val_meta, processor)\n",
    "        all_transcriptions[ckpt] = transcriptions\n",
    "    else:\n",
    "        print(f\"Checkpoint {ckpt} not found.\")\n",
    "        all_transcriptions[ckpt] = None\n",
    "\n",
    "# 결과 저장\n",
    "with open(\"transcription_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_transcriptions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== Transcription Results ===\")\n",
    "for ckpt, results in all_transcriptions.items():\n",
    "    print(f\"\\n--- Checkpoint: {ckpt} ---\")\n",
    "    for i, item in enumerate(results[:5]):  # 예시로 5개만 출력\n",
    "        print(f\"File: {item['file_name']}\")\n",
    "        print(f\"Reference: {item['reference']}\")\n",
    "        print(f\"Transcription: {item['transcription']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

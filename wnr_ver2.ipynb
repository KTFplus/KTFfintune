{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474be77-ffca-43e8-b8fb-0b0b85ace824",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터셋 스트리밍 로딩 및 통합\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# 두 데이터셋을 스트리밍으로 불러와 interleave로 합침\n",
    "#ds1 = load_dataset(\"jwh1449/AIhub_foreign_dataset\", split=\"train\", streaming=True)\n",
    "#ds2 = load_dataset(\"jwh1449/AIhub_foreign_dataset3\", split=\"train\", streaming=True)\n",
    "#dataset = interleave_datasets([ds1, ds2])\n",
    "\n",
    "dataset = load_dataset(\"jwh1449/AIhub_foreign_dataset3\", split=\"train\", streaming=True)\n",
    "\n",
    "# 오디오 컬럼이 'audio'일 경우 샘플링레이트 통일\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4faa795-caf7-44ae-a616-c3c4191a8bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 10:40:11.802530: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-11 10:40:11.846400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-11 10:40:12.544826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"ghost613/whisper-large-v3-turbo-korean\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  \n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cccf4-2693-4c2d-9540-ca048c5ec0a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 및 Whisper 입력 변환\n",
    "def prepare_features(example):\n",
    "    audio = example[\"audio\"]\n",
    "    text = example.get(\"text\") or example.get(\"transcripts\") or example.get(\"label\")\n",
    "    inputs = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # 텍스트 토크나이즈 및 -100 패딩\n",
    "    labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "    labels = torch.where(labels == processor.tokenizer.pad_token_id, -100, labels)\n",
    "    example[\"input_features\"] = inputs.input_features[0].to(torch.float32)\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_features = torch.stack([item[\"input_features\"] for item in batch])\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return {\"input_features\": input_features, \"labels\": labels_padded}\n",
    "\n",
    "def batch_iterator(dataset, batch_size=8):\n",
    "    batch = []\n",
    "    for example in dataset:\n",
    "        batch.append(prepare_features(example))\n",
    "        if len(batch) == batch_size:\n",
    "            yield collate_fn(batch)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42624a60-0682-4b48-8a82-7ae02b023c59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습 루프 (PEFT/LoRA, RTX 3090 기준)\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "save_dir = \"./model_saved\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_training_steps = 10000  \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=500, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 1. 데이터셋 샘플 확인\n",
    "try:\n",
    "    first = next(iter(dataset))\n",
    "    print(\"First sample:\", first)\n",
    "except Exception as e:\n",
    "    print(\"Error reading first sample:\", e)\n",
    "\n",
    "# 2. 배치 생성 확인\n",
    "for i, batch in enumerate(batch_iterator(dataset, batch_size=8)):\n",
    "    print(f\"Batch {i} loaded\", flush=True)\n",
    "    if i >= 2:\n",
    "        break\n",
    "\n",
    "# 3. 학습 루프 print에 flush 추가\n",
    "try:\n",
    "    for step, batch in enumerate(batch_iterator(dataset, batch_size=8)):\n",
    "        print(f\"Step {step} | Batch loaded\", flush=True)\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        print(\"Calculating outputs...\", flush=True)\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        print(\"Calculating loss...\", flush=True)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Step {step} | Loss: {loss.item():.4f}\", flush=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step} | Loss: {loss.item():.4f}\", flush=True)\n",
    "        if step % 200 == 0 and step > 0:\n",
    "            # LoRA 어댑터 저장 (Hugging Face 표준 포맷)\n",
    "            checkpoint_dir = f\"{save_dir}/lora_step{step}\"\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            processor.save_pretrained(checkpoint_dir)\n",
    "            print(f\"Checkpoint saved at step {step}: {checkpoint_dir}\", flush=True)\n",
    "        if step >= num_training_steps:\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(\"Training error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ae5af-760e-44e6-b648-46dd412d60d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "model.save_pretrained(\"./whisper-ktf-ver1\")\n",
    "processor.save_pretrained(\"./whisper-ktf-ver1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b95a-26a2-4898-8af4-7dcb80d6422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트(추론 및 WER/CER 평가)\n",
    "import evaluate\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def transcribe_and_evaluate(test_dataset, num_samples=100):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, example in enumerate(test_dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        audio = example[\"audio\"]\n",
    "        text = example.get(\"text\") or example.get(\"transcripts\") or example.get(\"label\")\n",
    "        inputs = processor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(inputs.input_features)\n",
    "        pred = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        preds.append(pred)\n",
    "        refs.append(text)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(\"  GT:\", text)\n",
    "            print(\"  STT:\", pred)\n",
    "    wer = wer_metric.compute(predictions=preds, references=refs)\n",
    "    cer = cer_metric.compute(predictions=preds, references=refs)\n",
    "    print(f\"\\n[TEST] WER: {wer*100:.2f}% | CER: {cer*100:.2f}%\")\n",
    "\n",
    "# 예시: validation split 100개로 테스트\n",
    "test_ds = load_dataset(\"jwh1449/AIhub_foreign_dataset\", split=\"validation\", streaming=True)\n",
    "test_ds = test_ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "transcribe_and_evaluate(test_ds, num_samples=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

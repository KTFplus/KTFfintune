{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de23699c-1ee9-4195-93bc-39834e011b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메모리 최적화 완료\n",
      "CSV 파일에서 데이터셋 생성 중...\n",
      "훈련 CSV 파일 로드 중: filtered_data_A.csv\n",
      "훈련 CSV 파일 로드 중: filtered_data_B.csv\n",
      "총 20623 개의 훈련 샘플 로드됨\n",
      "검증 CSV 파일 로드 중: filtered_data_val.csv\n",
      "총 576 개의 검증 샘플 로드됨\n",
      "결측치 제거 후 훈련 20623개, 검증 576개 샘플 남음\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (16/16 shards): 100%|██████████████████| 20623/20623 [00:52<00:00, 391.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████| 576/576 [00:00<00:00, 881.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 처리 완료 및 저장됨: ./whisper-korean-ft2/processed_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/vocab.json\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/tokenizer.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/merges.txt\n",
      "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/normalizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processor WhisperProcessor:\n",
      "- feature_extractor: WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "- tokenizer: WhisperTokenizer(name_or_path='openai/whisper-small', vocab_size=50258, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50257: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50258: AddedToken(\"<|startoftranscript|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50259: AddedToken(\"<|en|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50260: AddedToken(\"<|zh|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50261: AddedToken(\"<|de|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50262: AddedToken(\"<|es|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50263: AddedToken(\"<|ru|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50264: AddedToken(\"<|ko|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50265: AddedToken(\"<|fr|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50266: AddedToken(\"<|ja|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50267: AddedToken(\"<|pt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50268: AddedToken(\"<|tr|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50269: AddedToken(\"<|pl|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50270: AddedToken(\"<|ca|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50271: AddedToken(\"<|nl|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50272: AddedToken(\"<|ar|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50273: AddedToken(\"<|sv|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50274: AddedToken(\"<|it|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50275: AddedToken(\"<|id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50276: AddedToken(\"<|hi|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50277: AddedToken(\"<|fi|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50278: AddedToken(\"<|vi|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50279: AddedToken(\"<|he|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50280: AddedToken(\"<|uk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50281: AddedToken(\"<|el|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50282: AddedToken(\"<|ms|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50283: AddedToken(\"<|cs|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50284: AddedToken(\"<|ro|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50285: AddedToken(\"<|da|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50286: AddedToken(\"<|hu|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50287: AddedToken(\"<|ta|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50288: AddedToken(\"<|no|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50289: AddedToken(\"<|th|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50290: AddedToken(\"<|ur|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50291: AddedToken(\"<|hr|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50292: AddedToken(\"<|bg|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50293: AddedToken(\"<|lt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50294: AddedToken(\"<|la|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50295: AddedToken(\"<|mi|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50296: AddedToken(\"<|ml|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50297: AddedToken(\"<|cy|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50298: AddedToken(\"<|sk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50299: AddedToken(\"<|te|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50300: AddedToken(\"<|fa|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50301: AddedToken(\"<|lv|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50302: AddedToken(\"<|bn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50303: AddedToken(\"<|sr|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50304: AddedToken(\"<|az|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50305: AddedToken(\"<|sl|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50306: AddedToken(\"<|kn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50307: AddedToken(\"<|et|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50308: AddedToken(\"<|mk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50309: AddedToken(\"<|br|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50310: AddedToken(\"<|eu|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50311: AddedToken(\"<|is|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50312: AddedToken(\"<|hy|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50313: AddedToken(\"<|ne|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50314: AddedToken(\"<|mn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50315: AddedToken(\"<|bs|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50316: AddedToken(\"<|kk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50317: AddedToken(\"<|sq|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50318: AddedToken(\"<|sw|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50319: AddedToken(\"<|gl|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50320: AddedToken(\"<|mr|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50321: AddedToken(\"<|pa|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50322: AddedToken(\"<|si|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50323: AddedToken(\"<|km|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50324: AddedToken(\"<|sn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50325: AddedToken(\"<|yo|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50326: AddedToken(\"<|so|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50327: AddedToken(\"<|af|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50328: AddedToken(\"<|oc|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50329: AddedToken(\"<|ka|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50330: AddedToken(\"<|be|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50331: AddedToken(\"<|tg|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50332: AddedToken(\"<|sd|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50333: AddedToken(\"<|gu|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50334: AddedToken(\"<|am|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50335: AddedToken(\"<|yi|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50336: AddedToken(\"<|lo|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50337: AddedToken(\"<|uz|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50338: AddedToken(\"<|fo|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50339: AddedToken(\"<|ht|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50340: AddedToken(\"<|ps|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50341: AddedToken(\"<|tk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50342: AddedToken(\"<|nn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50343: AddedToken(\"<|mt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50344: AddedToken(\"<|sa|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50345: AddedToken(\"<|lb|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50346: AddedToken(\"<|my|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50347: AddedToken(\"<|bo|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50348: AddedToken(\"<|tl|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50349: AddedToken(\"<|mg|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50350: AddedToken(\"<|as|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50351: AddedToken(\"<|tt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50352: AddedToken(\"<|haw|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50353: AddedToken(\"<|ln|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50354: AddedToken(\"<|ha|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50355: AddedToken(\"<|ba|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50356: AddedToken(\"<|jw|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50357: AddedToken(\"<|su|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50358: AddedToken(\"<|translate|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50359: AddedToken(\"<|transcribe|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50360: AddedToken(\"<|startoflm|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50361: AddedToken(\"<|startofprev|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50362: AddedToken(\"<|nocaptions|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50363: AddedToken(\"<|notimestamps|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50364: AddedToken(\"<|0.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50365: AddedToken(\"<|0.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50366: AddedToken(\"<|0.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50367: AddedToken(\"<|0.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50368: AddedToken(\"<|0.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50369: AddedToken(\"<|0.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50370: AddedToken(\"<|0.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50371: AddedToken(\"<|0.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50372: AddedToken(\"<|0.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50373: AddedToken(\"<|0.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50374: AddedToken(\"<|0.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50375: AddedToken(\"<|0.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50376: AddedToken(\"<|0.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50377: AddedToken(\"<|0.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50378: AddedToken(\"<|0.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50379: AddedToken(\"<|0.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50380: AddedToken(\"<|0.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50381: AddedToken(\"<|0.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50382: AddedToken(\"<|0.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50383: AddedToken(\"<|0.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50384: AddedToken(\"<|0.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50385: AddedToken(\"<|0.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50386: AddedToken(\"<|0.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50387: AddedToken(\"<|0.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50388: AddedToken(\"<|0.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50389: AddedToken(\"<|0.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50390: AddedToken(\"<|0.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50391: AddedToken(\"<|0.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50392: AddedToken(\"<|0.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50393: AddedToken(\"<|0.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50394: AddedToken(\"<|0.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50395: AddedToken(\"<|0.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50396: AddedToken(\"<|0.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50397: AddedToken(\"<|0.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50398: AddedToken(\"<|0.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50399: AddedToken(\"<|0.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50400: AddedToken(\"<|0.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50401: AddedToken(\"<|0.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50402: AddedToken(\"<|0.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50403: AddedToken(\"<|0.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50404: AddedToken(\"<|0.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50405: AddedToken(\"<|0.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50406: AddedToken(\"<|0.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50407: AddedToken(\"<|0.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50408: AddedToken(\"<|0.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50409: AddedToken(\"<|0.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50410: AddedToken(\"<|0.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50411: AddedToken(\"<|0.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50412: AddedToken(\"<|0.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50413: AddedToken(\"<|0.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50414: AddedToken(\"<|1.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50415: AddedToken(\"<|1.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50416: AddedToken(\"<|1.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50417: AddedToken(\"<|1.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50418: AddedToken(\"<|1.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50419: AddedToken(\"<|1.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50420: AddedToken(\"<|1.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50421: AddedToken(\"<|1.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50422: AddedToken(\"<|1.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50423: AddedToken(\"<|1.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50424: AddedToken(\"<|1.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50425: AddedToken(\"<|1.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50426: AddedToken(\"<|1.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50427: AddedToken(\"<|1.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50428: AddedToken(\"<|1.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50429: AddedToken(\"<|1.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50430: AddedToken(\"<|1.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50431: AddedToken(\"<|1.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50432: AddedToken(\"<|1.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50433: AddedToken(\"<|1.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50434: AddedToken(\"<|1.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50435: AddedToken(\"<|1.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50436: AddedToken(\"<|1.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50437: AddedToken(\"<|1.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50438: AddedToken(\"<|1.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50439: AddedToken(\"<|1.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50440: AddedToken(\"<|1.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50441: AddedToken(\"<|1.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50442: AddedToken(\"<|1.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50443: AddedToken(\"<|1.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50444: AddedToken(\"<|1.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50445: AddedToken(\"<|1.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50446: AddedToken(\"<|1.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50447: AddedToken(\"<|1.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50448: AddedToken(\"<|1.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50449: AddedToken(\"<|1.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50450: AddedToken(\"<|1.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50451: AddedToken(\"<|1.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50452: AddedToken(\"<|1.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50453: AddedToken(\"<|1.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50454: AddedToken(\"<|1.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50455: AddedToken(\"<|1.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50456: AddedToken(\"<|1.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50457: AddedToken(\"<|1.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50458: AddedToken(\"<|1.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50459: AddedToken(\"<|1.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50460: AddedToken(\"<|1.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50461: AddedToken(\"<|1.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50462: AddedToken(\"<|1.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50463: AddedToken(\"<|1.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50464: AddedToken(\"<|2.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50465: AddedToken(\"<|2.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50466: AddedToken(\"<|2.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50467: AddedToken(\"<|2.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50468: AddedToken(\"<|2.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50469: AddedToken(\"<|2.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50470: AddedToken(\"<|2.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50471: AddedToken(\"<|2.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50472: AddedToken(\"<|2.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50473: AddedToken(\"<|2.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50474: AddedToken(\"<|2.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50475: AddedToken(\"<|2.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50476: AddedToken(\"<|2.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50477: AddedToken(\"<|2.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50478: AddedToken(\"<|2.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50479: AddedToken(\"<|2.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50480: AddedToken(\"<|2.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50481: AddedToken(\"<|2.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50482: AddedToken(\"<|2.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50483: AddedToken(\"<|2.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50484: AddedToken(\"<|2.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50485: AddedToken(\"<|2.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50486: AddedToken(\"<|2.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50487: AddedToken(\"<|2.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50488: AddedToken(\"<|2.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50489: AddedToken(\"<|2.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50490: AddedToken(\"<|2.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50491: AddedToken(\"<|2.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50492: AddedToken(\"<|2.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50493: AddedToken(\"<|2.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50494: AddedToken(\"<|2.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50495: AddedToken(\"<|2.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50496: AddedToken(\"<|2.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50497: AddedToken(\"<|2.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50498: AddedToken(\"<|2.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50499: AddedToken(\"<|2.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50500: AddedToken(\"<|2.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50501: AddedToken(\"<|2.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50502: AddedToken(\"<|2.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50503: AddedToken(\"<|2.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50504: AddedToken(\"<|2.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50505: AddedToken(\"<|2.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50506: AddedToken(\"<|2.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50507: AddedToken(\"<|2.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50508: AddedToken(\"<|2.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50509: AddedToken(\"<|2.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50510: AddedToken(\"<|2.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50511: AddedToken(\"<|2.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50512: AddedToken(\"<|2.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50513: AddedToken(\"<|2.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50514: AddedToken(\"<|3.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50515: AddedToken(\"<|3.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50516: AddedToken(\"<|3.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50517: AddedToken(\"<|3.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50518: AddedToken(\"<|3.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50519: AddedToken(\"<|3.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50520: AddedToken(\"<|3.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50521: AddedToken(\"<|3.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50522: AddedToken(\"<|3.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50523: AddedToken(\"<|3.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50524: AddedToken(\"<|3.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50525: AddedToken(\"<|3.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50526: AddedToken(\"<|3.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50527: AddedToken(\"<|3.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50528: AddedToken(\"<|3.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50529: AddedToken(\"<|3.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50530: AddedToken(\"<|3.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50531: AddedToken(\"<|3.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50532: AddedToken(\"<|3.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50533: AddedToken(\"<|3.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50534: AddedToken(\"<|3.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50535: AddedToken(\"<|3.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50536: AddedToken(\"<|3.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50537: AddedToken(\"<|3.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50538: AddedToken(\"<|3.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50539: AddedToken(\"<|3.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50540: AddedToken(\"<|3.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50541: AddedToken(\"<|3.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50542: AddedToken(\"<|3.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50543: AddedToken(\"<|3.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50544: AddedToken(\"<|3.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50545: AddedToken(\"<|3.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50546: AddedToken(\"<|3.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50547: AddedToken(\"<|3.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50548: AddedToken(\"<|3.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50549: AddedToken(\"<|3.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50550: AddedToken(\"<|3.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50551: AddedToken(\"<|3.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50552: AddedToken(\"<|3.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50553: AddedToken(\"<|3.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50554: AddedToken(\"<|3.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50555: AddedToken(\"<|3.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50556: AddedToken(\"<|3.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50557: AddedToken(\"<|3.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50558: AddedToken(\"<|3.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50559: AddedToken(\"<|3.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50560: AddedToken(\"<|3.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50561: AddedToken(\"<|3.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50562: AddedToken(\"<|3.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50563: AddedToken(\"<|3.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50564: AddedToken(\"<|4.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50565: AddedToken(\"<|4.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50566: AddedToken(\"<|4.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50567: AddedToken(\"<|4.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50568: AddedToken(\"<|4.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50569: AddedToken(\"<|4.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50570: AddedToken(\"<|4.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50571: AddedToken(\"<|4.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50572: AddedToken(\"<|4.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50573: AddedToken(\"<|4.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50574: AddedToken(\"<|4.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50575: AddedToken(\"<|4.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50576: AddedToken(\"<|4.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50577: AddedToken(\"<|4.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50578: AddedToken(\"<|4.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50579: AddedToken(\"<|4.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50580: AddedToken(\"<|4.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50581: AddedToken(\"<|4.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50582: AddedToken(\"<|4.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50583: AddedToken(\"<|4.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50584: AddedToken(\"<|4.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50585: AddedToken(\"<|4.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50586: AddedToken(\"<|4.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50587: AddedToken(\"<|4.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50588: AddedToken(\"<|4.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50589: AddedToken(\"<|4.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50590: AddedToken(\"<|4.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50591: AddedToken(\"<|4.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50592: AddedToken(\"<|4.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50593: AddedToken(\"<|4.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50594: AddedToken(\"<|4.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50595: AddedToken(\"<|4.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50596: AddedToken(\"<|4.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50597: AddedToken(\"<|4.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50598: AddedToken(\"<|4.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50599: AddedToken(\"<|4.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50600: AddedToken(\"<|4.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50601: AddedToken(\"<|4.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50602: AddedToken(\"<|4.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50603: AddedToken(\"<|4.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50604: AddedToken(\"<|4.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50605: AddedToken(\"<|4.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50606: AddedToken(\"<|4.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50607: AddedToken(\"<|4.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50608: AddedToken(\"<|4.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50609: AddedToken(\"<|4.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50610: AddedToken(\"<|4.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50611: AddedToken(\"<|4.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50612: AddedToken(\"<|4.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50613: AddedToken(\"<|4.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50614: AddedToken(\"<|5.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50615: AddedToken(\"<|5.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50616: AddedToken(\"<|5.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50617: AddedToken(\"<|5.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50618: AddedToken(\"<|5.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50619: AddedToken(\"<|5.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50620: AddedToken(\"<|5.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50621: AddedToken(\"<|5.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50622: AddedToken(\"<|5.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50623: AddedToken(\"<|5.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50624: AddedToken(\"<|5.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50625: AddedToken(\"<|5.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50626: AddedToken(\"<|5.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50627: AddedToken(\"<|5.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50628: AddedToken(\"<|5.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50629: AddedToken(\"<|5.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50630: AddedToken(\"<|5.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50631: AddedToken(\"<|5.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50632: AddedToken(\"<|5.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50633: AddedToken(\"<|5.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50634: AddedToken(\"<|5.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50635: AddedToken(\"<|5.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50636: AddedToken(\"<|5.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50637: AddedToken(\"<|5.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50638: AddedToken(\"<|5.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50639: AddedToken(\"<|5.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50640: AddedToken(\"<|5.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50641: AddedToken(\"<|5.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50642: AddedToken(\"<|5.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50643: AddedToken(\"<|5.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50644: AddedToken(\"<|5.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50645: AddedToken(\"<|5.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50646: AddedToken(\"<|5.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50647: AddedToken(\"<|5.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50648: AddedToken(\"<|5.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50649: AddedToken(\"<|5.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50650: AddedToken(\"<|5.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50651: AddedToken(\"<|5.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50652: AddedToken(\"<|5.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50653: AddedToken(\"<|5.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50654: AddedToken(\"<|5.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50655: AddedToken(\"<|5.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50656: AddedToken(\"<|5.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50657: AddedToken(\"<|5.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50658: AddedToken(\"<|5.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50659: AddedToken(\"<|5.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50660: AddedToken(\"<|5.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50661: AddedToken(\"<|5.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50662: AddedToken(\"<|5.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50663: AddedToken(\"<|5.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50664: AddedToken(\"<|6.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50665: AddedToken(\"<|6.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50666: AddedToken(\"<|6.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50667: AddedToken(\"<|6.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50668: AddedToken(\"<|6.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50669: AddedToken(\"<|6.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50670: AddedToken(\"<|6.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50671: AddedToken(\"<|6.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50672: AddedToken(\"<|6.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50673: AddedToken(\"<|6.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50674: AddedToken(\"<|6.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50675: AddedToken(\"<|6.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50676: AddedToken(\"<|6.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50677: AddedToken(\"<|6.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50678: AddedToken(\"<|6.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50679: AddedToken(\"<|6.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50680: AddedToken(\"<|6.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50681: AddedToken(\"<|6.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50682: AddedToken(\"<|6.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50683: AddedToken(\"<|6.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50684: AddedToken(\"<|6.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50685: AddedToken(\"<|6.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50686: AddedToken(\"<|6.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50687: AddedToken(\"<|6.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50688: AddedToken(\"<|6.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50689: AddedToken(\"<|6.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50690: AddedToken(\"<|6.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50691: AddedToken(\"<|6.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50692: AddedToken(\"<|6.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50693: AddedToken(\"<|6.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50694: AddedToken(\"<|6.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50695: AddedToken(\"<|6.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50696: AddedToken(\"<|6.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50697: AddedToken(\"<|6.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50698: AddedToken(\"<|6.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50699: AddedToken(\"<|6.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50700: AddedToken(\"<|6.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50701: AddedToken(\"<|6.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50702: AddedToken(\"<|6.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50703: AddedToken(\"<|6.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50704: AddedToken(\"<|6.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50705: AddedToken(\"<|6.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50706: AddedToken(\"<|6.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50707: AddedToken(\"<|6.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50708: AddedToken(\"<|6.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50709: AddedToken(\"<|6.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50710: AddedToken(\"<|6.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50711: AddedToken(\"<|6.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50712: AddedToken(\"<|6.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50713: AddedToken(\"<|6.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50714: AddedToken(\"<|7.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50715: AddedToken(\"<|7.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50716: AddedToken(\"<|7.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50717: AddedToken(\"<|7.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50718: AddedToken(\"<|7.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50719: AddedToken(\"<|7.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50720: AddedToken(\"<|7.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50721: AddedToken(\"<|7.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50722: AddedToken(\"<|7.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50723: AddedToken(\"<|7.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50724: AddedToken(\"<|7.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50725: AddedToken(\"<|7.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50726: AddedToken(\"<|7.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50727: AddedToken(\"<|7.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50728: AddedToken(\"<|7.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50729: AddedToken(\"<|7.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50730: AddedToken(\"<|7.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50731: AddedToken(\"<|7.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50732: AddedToken(\"<|7.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50733: AddedToken(\"<|7.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50734: AddedToken(\"<|7.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50735: AddedToken(\"<|7.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50736: AddedToken(\"<|7.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50737: AddedToken(\"<|7.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50738: AddedToken(\"<|7.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50739: AddedToken(\"<|7.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50740: AddedToken(\"<|7.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50741: AddedToken(\"<|7.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50742: AddedToken(\"<|7.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50743: AddedToken(\"<|7.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50744: AddedToken(\"<|7.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50745: AddedToken(\"<|7.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50746: AddedToken(\"<|7.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50747: AddedToken(\"<|7.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50748: AddedToken(\"<|7.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50749: AddedToken(\"<|7.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50750: AddedToken(\"<|7.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50751: AddedToken(\"<|7.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50752: AddedToken(\"<|7.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50753: AddedToken(\"<|7.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50754: AddedToken(\"<|7.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50755: AddedToken(\"<|7.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50756: AddedToken(\"<|7.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50757: AddedToken(\"<|7.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50758: AddedToken(\"<|7.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50759: AddedToken(\"<|7.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50760: AddedToken(\"<|7.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50761: AddedToken(\"<|7.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50762: AddedToken(\"<|7.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50763: AddedToken(\"<|7.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50764: AddedToken(\"<|8.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50765: AddedToken(\"<|8.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50766: AddedToken(\"<|8.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50767: AddedToken(\"<|8.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50768: AddedToken(\"<|8.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50769: AddedToken(\"<|8.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50770: AddedToken(\"<|8.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50771: AddedToken(\"<|8.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50772: AddedToken(\"<|8.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50773: AddedToken(\"<|8.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50774: AddedToken(\"<|8.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50775: AddedToken(\"<|8.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50776: AddedToken(\"<|8.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50777: AddedToken(\"<|8.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50778: AddedToken(\"<|8.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50779: AddedToken(\"<|8.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50780: AddedToken(\"<|8.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50781: AddedToken(\"<|8.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50782: AddedToken(\"<|8.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50783: AddedToken(\"<|8.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50784: AddedToken(\"<|8.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50785: AddedToken(\"<|8.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50786: AddedToken(\"<|8.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50787: AddedToken(\"<|8.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50788: AddedToken(\"<|8.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50789: AddedToken(\"<|8.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50790: AddedToken(\"<|8.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50791: AddedToken(\"<|8.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50792: AddedToken(\"<|8.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50793: AddedToken(\"<|8.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50794: AddedToken(\"<|8.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50795: AddedToken(\"<|8.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50796: AddedToken(\"<|8.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50797: AddedToken(\"<|8.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50798: AddedToken(\"<|8.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50799: AddedToken(\"<|8.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50800: AddedToken(\"<|8.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50801: AddedToken(\"<|8.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50802: AddedToken(\"<|8.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50803: AddedToken(\"<|8.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50804: AddedToken(\"<|8.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50805: AddedToken(\"<|8.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50806: AddedToken(\"<|8.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50807: AddedToken(\"<|8.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50808: AddedToken(\"<|8.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50809: AddedToken(\"<|8.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50810: AddedToken(\"<|8.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50811: AddedToken(\"<|8.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50812: AddedToken(\"<|8.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50813: AddedToken(\"<|8.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50814: AddedToken(\"<|9.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50815: AddedToken(\"<|9.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50816: AddedToken(\"<|9.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50817: AddedToken(\"<|9.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50818: AddedToken(\"<|9.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50819: AddedToken(\"<|9.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50820: AddedToken(\"<|9.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50821: AddedToken(\"<|9.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50822: AddedToken(\"<|9.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50823: AddedToken(\"<|9.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50824: AddedToken(\"<|9.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50825: AddedToken(\"<|9.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50826: AddedToken(\"<|9.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50827: AddedToken(\"<|9.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50828: AddedToken(\"<|9.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50829: AddedToken(\"<|9.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50830: AddedToken(\"<|9.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50831: AddedToken(\"<|9.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50832: AddedToken(\"<|9.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50833: AddedToken(\"<|9.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50834: AddedToken(\"<|9.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50835: AddedToken(\"<|9.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50836: AddedToken(\"<|9.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50837: AddedToken(\"<|9.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50838: AddedToken(\"<|9.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50839: AddedToken(\"<|9.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50840: AddedToken(\"<|9.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50841: AddedToken(\"<|9.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50842: AddedToken(\"<|9.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50843: AddedToken(\"<|9.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50844: AddedToken(\"<|9.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50845: AddedToken(\"<|9.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50846: AddedToken(\"<|9.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50847: AddedToken(\"<|9.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50848: AddedToken(\"<|9.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50849: AddedToken(\"<|9.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50850: AddedToken(\"<|9.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50851: AddedToken(\"<|9.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50852: AddedToken(\"<|9.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50853: AddedToken(\"<|9.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50854: AddedToken(\"<|9.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50855: AddedToken(\"<|9.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50856: AddedToken(\"<|9.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50857: AddedToken(\"<|9.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50858: AddedToken(\"<|9.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50859: AddedToken(\"<|9.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50860: AddedToken(\"<|9.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50861: AddedToken(\"<|9.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50862: AddedToken(\"<|9.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50863: AddedToken(\"<|9.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50864: AddedToken(\"<|10.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50865: AddedToken(\"<|10.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50866: AddedToken(\"<|10.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50867: AddedToken(\"<|10.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50868: AddedToken(\"<|10.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50869: AddedToken(\"<|10.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50870: AddedToken(\"<|10.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50871: AddedToken(\"<|10.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50872: AddedToken(\"<|10.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50873: AddedToken(\"<|10.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50874: AddedToken(\"<|10.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50875: AddedToken(\"<|10.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50876: AddedToken(\"<|10.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50877: AddedToken(\"<|10.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50878: AddedToken(\"<|10.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50879: AddedToken(\"<|10.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50880: AddedToken(\"<|10.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50881: AddedToken(\"<|10.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50882: AddedToken(\"<|10.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50883: AddedToken(\"<|10.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50884: AddedToken(\"<|10.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50885: AddedToken(\"<|10.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50886: AddedToken(\"<|10.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50887: AddedToken(\"<|10.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50888: AddedToken(\"<|10.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50889: AddedToken(\"<|10.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50890: AddedToken(\"<|10.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50891: AddedToken(\"<|10.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50892: AddedToken(\"<|10.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50893: AddedToken(\"<|10.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50894: AddedToken(\"<|10.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50895: AddedToken(\"<|10.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50896: AddedToken(\"<|10.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50897: AddedToken(\"<|10.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50898: AddedToken(\"<|10.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50899: AddedToken(\"<|10.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50900: AddedToken(\"<|10.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50901: AddedToken(\"<|10.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50902: AddedToken(\"<|10.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50903: AddedToken(\"<|10.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50904: AddedToken(\"<|10.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50905: AddedToken(\"<|10.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50906: AddedToken(\"<|10.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50907: AddedToken(\"<|10.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50908: AddedToken(\"<|10.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50909: AddedToken(\"<|10.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50910: AddedToken(\"<|10.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50911: AddedToken(\"<|10.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50912: AddedToken(\"<|10.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50913: AddedToken(\"<|10.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50914: AddedToken(\"<|11.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50915: AddedToken(\"<|11.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50916: AddedToken(\"<|11.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50917: AddedToken(\"<|11.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50918: AddedToken(\"<|11.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50919: AddedToken(\"<|11.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50920: AddedToken(\"<|11.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50921: AddedToken(\"<|11.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50922: AddedToken(\"<|11.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50923: AddedToken(\"<|11.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50924: AddedToken(\"<|11.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50925: AddedToken(\"<|11.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50926: AddedToken(\"<|11.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50927: AddedToken(\"<|11.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50928: AddedToken(\"<|11.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50929: AddedToken(\"<|11.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50930: AddedToken(\"<|11.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50931: AddedToken(\"<|11.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50932: AddedToken(\"<|11.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50933: AddedToken(\"<|11.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50934: AddedToken(\"<|11.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50935: AddedToken(\"<|11.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50936: AddedToken(\"<|11.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50937: AddedToken(\"<|11.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50938: AddedToken(\"<|11.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50939: AddedToken(\"<|11.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50940: AddedToken(\"<|11.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50941: AddedToken(\"<|11.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50942: AddedToken(\"<|11.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50943: AddedToken(\"<|11.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50944: AddedToken(\"<|11.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50945: AddedToken(\"<|11.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50946: AddedToken(\"<|11.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50947: AddedToken(\"<|11.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50948: AddedToken(\"<|11.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50949: AddedToken(\"<|11.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50950: AddedToken(\"<|11.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50951: AddedToken(\"<|11.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50952: AddedToken(\"<|11.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50953: AddedToken(\"<|11.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50954: AddedToken(\"<|11.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50955: AddedToken(\"<|11.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50956: AddedToken(\"<|11.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50957: AddedToken(\"<|11.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50958: AddedToken(\"<|11.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50959: AddedToken(\"<|11.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50960: AddedToken(\"<|11.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50961: AddedToken(\"<|11.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50962: AddedToken(\"<|11.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50963: AddedToken(\"<|11.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50964: AddedToken(\"<|12.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50965: AddedToken(\"<|12.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50966: AddedToken(\"<|12.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50967: AddedToken(\"<|12.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50968: AddedToken(\"<|12.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50969: AddedToken(\"<|12.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50970: AddedToken(\"<|12.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50971: AddedToken(\"<|12.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50972: AddedToken(\"<|12.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50973: AddedToken(\"<|12.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50974: AddedToken(\"<|12.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50975: AddedToken(\"<|12.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50976: AddedToken(\"<|12.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50977: AddedToken(\"<|12.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50978: AddedToken(\"<|12.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50979: AddedToken(\"<|12.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50980: AddedToken(\"<|12.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50981: AddedToken(\"<|12.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50982: AddedToken(\"<|12.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50983: AddedToken(\"<|12.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50984: AddedToken(\"<|12.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50985: AddedToken(\"<|12.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50986: AddedToken(\"<|12.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50987: AddedToken(\"<|12.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50988: AddedToken(\"<|12.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50989: AddedToken(\"<|12.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50990: AddedToken(\"<|12.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50991: AddedToken(\"<|12.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50992: AddedToken(\"<|12.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50993: AddedToken(\"<|12.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50994: AddedToken(\"<|12.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50995: AddedToken(\"<|12.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50996: AddedToken(\"<|12.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50997: AddedToken(\"<|12.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50998: AddedToken(\"<|12.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50999: AddedToken(\"<|12.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51000: AddedToken(\"<|12.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51001: AddedToken(\"<|12.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51002: AddedToken(\"<|12.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51003: AddedToken(\"<|12.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51004: AddedToken(\"<|12.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51005: AddedToken(\"<|12.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51006: AddedToken(\"<|12.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51007: AddedToken(\"<|12.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51008: AddedToken(\"<|12.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51009: AddedToken(\"<|12.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51010: AddedToken(\"<|12.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51011: AddedToken(\"<|12.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51012: AddedToken(\"<|12.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51013: AddedToken(\"<|12.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51014: AddedToken(\"<|13.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51015: AddedToken(\"<|13.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51016: AddedToken(\"<|13.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51017: AddedToken(\"<|13.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51018: AddedToken(\"<|13.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51019: AddedToken(\"<|13.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51020: AddedToken(\"<|13.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51021: AddedToken(\"<|13.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51022: AddedToken(\"<|13.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51023: AddedToken(\"<|13.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51024: AddedToken(\"<|13.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51025: AddedToken(\"<|13.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51026: AddedToken(\"<|13.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51027: AddedToken(\"<|13.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51028: AddedToken(\"<|13.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51029: AddedToken(\"<|13.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51030: AddedToken(\"<|13.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51031: AddedToken(\"<|13.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51032: AddedToken(\"<|13.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51033: AddedToken(\"<|13.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51034: AddedToken(\"<|13.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51035: AddedToken(\"<|13.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51036: AddedToken(\"<|13.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51037: AddedToken(\"<|13.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51038: AddedToken(\"<|13.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51039: AddedToken(\"<|13.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51040: AddedToken(\"<|13.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51041: AddedToken(\"<|13.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51042: AddedToken(\"<|13.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51043: AddedToken(\"<|13.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51044: AddedToken(\"<|13.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51045: AddedToken(\"<|13.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51046: AddedToken(\"<|13.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51047: AddedToken(\"<|13.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51048: AddedToken(\"<|13.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51049: AddedToken(\"<|13.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51050: AddedToken(\"<|13.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51051: AddedToken(\"<|13.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51052: AddedToken(\"<|13.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51053: AddedToken(\"<|13.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51054: AddedToken(\"<|13.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51055: AddedToken(\"<|13.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51056: AddedToken(\"<|13.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51057: AddedToken(\"<|13.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51058: AddedToken(\"<|13.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51059: AddedToken(\"<|13.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51060: AddedToken(\"<|13.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51061: AddedToken(\"<|13.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51062: AddedToken(\"<|13.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51063: AddedToken(\"<|13.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51064: AddedToken(\"<|14.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51065: AddedToken(\"<|14.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51066: AddedToken(\"<|14.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51067: AddedToken(\"<|14.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51068: AddedToken(\"<|14.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51069: AddedToken(\"<|14.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51070: AddedToken(\"<|14.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51071: AddedToken(\"<|14.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51072: AddedToken(\"<|14.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51073: AddedToken(\"<|14.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51074: AddedToken(\"<|14.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51075: AddedToken(\"<|14.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51076: AddedToken(\"<|14.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51077: AddedToken(\"<|14.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51078: AddedToken(\"<|14.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51079: AddedToken(\"<|14.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51080: AddedToken(\"<|14.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51081: AddedToken(\"<|14.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51082: AddedToken(\"<|14.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51083: AddedToken(\"<|14.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51084: AddedToken(\"<|14.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51085: AddedToken(\"<|14.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51086: AddedToken(\"<|14.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51087: AddedToken(\"<|14.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51088: AddedToken(\"<|14.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51089: AddedToken(\"<|14.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51090: AddedToken(\"<|14.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51091: AddedToken(\"<|14.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51092: AddedToken(\"<|14.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51093: AddedToken(\"<|14.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51094: AddedToken(\"<|14.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51095: AddedToken(\"<|14.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51096: AddedToken(\"<|14.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51097: AddedToken(\"<|14.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51098: AddedToken(\"<|14.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51099: AddedToken(\"<|14.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51100: AddedToken(\"<|14.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51101: AddedToken(\"<|14.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51102: AddedToken(\"<|14.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51103: AddedToken(\"<|14.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51104: AddedToken(\"<|14.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51105: AddedToken(\"<|14.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51106: AddedToken(\"<|14.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51107: AddedToken(\"<|14.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51108: AddedToken(\"<|14.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51109: AddedToken(\"<|14.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51110: AddedToken(\"<|14.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51111: AddedToken(\"<|14.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51112: AddedToken(\"<|14.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51113: AddedToken(\"<|14.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51114: AddedToken(\"<|15.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51115: AddedToken(\"<|15.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51116: AddedToken(\"<|15.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51117: AddedToken(\"<|15.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51118: AddedToken(\"<|15.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51119: AddedToken(\"<|15.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51120: AddedToken(\"<|15.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51121: AddedToken(\"<|15.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51122: AddedToken(\"<|15.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51123: AddedToken(\"<|15.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51124: AddedToken(\"<|15.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51125: AddedToken(\"<|15.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51126: AddedToken(\"<|15.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51127: AddedToken(\"<|15.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51128: AddedToken(\"<|15.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51129: AddedToken(\"<|15.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51130: AddedToken(\"<|15.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51131: AddedToken(\"<|15.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51132: AddedToken(\"<|15.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51133: AddedToken(\"<|15.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51134: AddedToken(\"<|15.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51135: AddedToken(\"<|15.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51136: AddedToken(\"<|15.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51137: AddedToken(\"<|15.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51138: AddedToken(\"<|15.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51139: AddedToken(\"<|15.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51140: AddedToken(\"<|15.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51141: AddedToken(\"<|15.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51142: AddedToken(\"<|15.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51143: AddedToken(\"<|15.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51144: AddedToken(\"<|15.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51145: AddedToken(\"<|15.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51146: AddedToken(\"<|15.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51147: AddedToken(\"<|15.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51148: AddedToken(\"<|15.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51149: AddedToken(\"<|15.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51150: AddedToken(\"<|15.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51151: AddedToken(\"<|15.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51152: AddedToken(\"<|15.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51153: AddedToken(\"<|15.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51154: AddedToken(\"<|15.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51155: AddedToken(\"<|15.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51156: AddedToken(\"<|15.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51157: AddedToken(\"<|15.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51158: AddedToken(\"<|15.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51159: AddedToken(\"<|15.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51160: AddedToken(\"<|15.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51161: AddedToken(\"<|15.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51162: AddedToken(\"<|15.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51163: AddedToken(\"<|15.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51164: AddedToken(\"<|16.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51165: AddedToken(\"<|16.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51166: AddedToken(\"<|16.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51167: AddedToken(\"<|16.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51168: AddedToken(\"<|16.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51169: AddedToken(\"<|16.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51170: AddedToken(\"<|16.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51171: AddedToken(\"<|16.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51172: AddedToken(\"<|16.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51173: AddedToken(\"<|16.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51174: AddedToken(\"<|16.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51175: AddedToken(\"<|16.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51176: AddedToken(\"<|16.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51177: AddedToken(\"<|16.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51178: AddedToken(\"<|16.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51179: AddedToken(\"<|16.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51180: AddedToken(\"<|16.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51181: AddedToken(\"<|16.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51182: AddedToken(\"<|16.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51183: AddedToken(\"<|16.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51184: AddedToken(\"<|16.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51185: AddedToken(\"<|16.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51186: AddedToken(\"<|16.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51187: AddedToken(\"<|16.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51188: AddedToken(\"<|16.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51189: AddedToken(\"<|16.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51190: AddedToken(\"<|16.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51191: AddedToken(\"<|16.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51192: AddedToken(\"<|16.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51193: AddedToken(\"<|16.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51194: AddedToken(\"<|16.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51195: AddedToken(\"<|16.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51196: AddedToken(\"<|16.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51197: AddedToken(\"<|16.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51198: AddedToken(\"<|16.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51199: AddedToken(\"<|16.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51200: AddedToken(\"<|16.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51201: AddedToken(\"<|16.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51202: AddedToken(\"<|16.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51203: AddedToken(\"<|16.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51204: AddedToken(\"<|16.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51205: AddedToken(\"<|16.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51206: AddedToken(\"<|16.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51207: AddedToken(\"<|16.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51208: AddedToken(\"<|16.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51209: AddedToken(\"<|16.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51210: AddedToken(\"<|16.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51211: AddedToken(\"<|16.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51212: AddedToken(\"<|16.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51213: AddedToken(\"<|16.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51214: AddedToken(\"<|17.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51215: AddedToken(\"<|17.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51216: AddedToken(\"<|17.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51217: AddedToken(\"<|17.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51218: AddedToken(\"<|17.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51219: AddedToken(\"<|17.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51220: AddedToken(\"<|17.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51221: AddedToken(\"<|17.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51222: AddedToken(\"<|17.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51223: AddedToken(\"<|17.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51224: AddedToken(\"<|17.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51225: AddedToken(\"<|17.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51226: AddedToken(\"<|17.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51227: AddedToken(\"<|17.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51228: AddedToken(\"<|17.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51229: AddedToken(\"<|17.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51230: AddedToken(\"<|17.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51231: AddedToken(\"<|17.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51232: AddedToken(\"<|17.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51233: AddedToken(\"<|17.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51234: AddedToken(\"<|17.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51235: AddedToken(\"<|17.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51236: AddedToken(\"<|17.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51237: AddedToken(\"<|17.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51238: AddedToken(\"<|17.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51239: AddedToken(\"<|17.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51240: AddedToken(\"<|17.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51241: AddedToken(\"<|17.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51242: AddedToken(\"<|17.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51243: AddedToken(\"<|17.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51244: AddedToken(\"<|17.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51245: AddedToken(\"<|17.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51246: AddedToken(\"<|17.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51247: AddedToken(\"<|17.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51248: AddedToken(\"<|17.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51249: AddedToken(\"<|17.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51250: AddedToken(\"<|17.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51251: AddedToken(\"<|17.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51252: AddedToken(\"<|17.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51253: AddedToken(\"<|17.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51254: AddedToken(\"<|17.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51255: AddedToken(\"<|17.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51256: AddedToken(\"<|17.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51257: AddedToken(\"<|17.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51258: AddedToken(\"<|17.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51259: AddedToken(\"<|17.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51260: AddedToken(\"<|17.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51261: AddedToken(\"<|17.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51262: AddedToken(\"<|17.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51263: AddedToken(\"<|17.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51264: AddedToken(\"<|18.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51265: AddedToken(\"<|18.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51266: AddedToken(\"<|18.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51267: AddedToken(\"<|18.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51268: AddedToken(\"<|18.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51269: AddedToken(\"<|18.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51270: AddedToken(\"<|18.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51271: AddedToken(\"<|18.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51272: AddedToken(\"<|18.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51273: AddedToken(\"<|18.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51274: AddedToken(\"<|18.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51275: AddedToken(\"<|18.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51276: AddedToken(\"<|18.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51277: AddedToken(\"<|18.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51278: AddedToken(\"<|18.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51279: AddedToken(\"<|18.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51280: AddedToken(\"<|18.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51281: AddedToken(\"<|18.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51282: AddedToken(\"<|18.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51283: AddedToken(\"<|18.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51284: AddedToken(\"<|18.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51285: AddedToken(\"<|18.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51286: AddedToken(\"<|18.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51287: AddedToken(\"<|18.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51288: AddedToken(\"<|18.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51289: AddedToken(\"<|18.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51290: AddedToken(\"<|18.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51291: AddedToken(\"<|18.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51292: AddedToken(\"<|18.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51293: AddedToken(\"<|18.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51294: AddedToken(\"<|18.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51295: AddedToken(\"<|18.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51296: AddedToken(\"<|18.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51297: AddedToken(\"<|18.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51298: AddedToken(\"<|18.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51299: AddedToken(\"<|18.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51300: AddedToken(\"<|18.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51301: AddedToken(\"<|18.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51302: AddedToken(\"<|18.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51303: AddedToken(\"<|18.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51304: AddedToken(\"<|18.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51305: AddedToken(\"<|18.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51306: AddedToken(\"<|18.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51307: AddedToken(\"<|18.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51308: AddedToken(\"<|18.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51309: AddedToken(\"<|18.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51310: AddedToken(\"<|18.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51311: AddedToken(\"<|18.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51312: AddedToken(\"<|18.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51313: AddedToken(\"<|18.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51314: AddedToken(\"<|19.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51315: AddedToken(\"<|19.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51316: AddedToken(\"<|19.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51317: AddedToken(\"<|19.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51318: AddedToken(\"<|19.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51319: AddedToken(\"<|19.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51320: AddedToken(\"<|19.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51321: AddedToken(\"<|19.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51322: AddedToken(\"<|19.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51323: AddedToken(\"<|19.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51324: AddedToken(\"<|19.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51325: AddedToken(\"<|19.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51326: AddedToken(\"<|19.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51327: AddedToken(\"<|19.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51328: AddedToken(\"<|19.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51329: AddedToken(\"<|19.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51330: AddedToken(\"<|19.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51331: AddedToken(\"<|19.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51332: AddedToken(\"<|19.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51333: AddedToken(\"<|19.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51334: AddedToken(\"<|19.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51335: AddedToken(\"<|19.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51336: AddedToken(\"<|19.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51337: AddedToken(\"<|19.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51338: AddedToken(\"<|19.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51339: AddedToken(\"<|19.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51340: AddedToken(\"<|19.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51341: AddedToken(\"<|19.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51342: AddedToken(\"<|19.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51343: AddedToken(\"<|19.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51344: AddedToken(\"<|19.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51345: AddedToken(\"<|19.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51346: AddedToken(\"<|19.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51347: AddedToken(\"<|19.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51348: AddedToken(\"<|19.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51349: AddedToken(\"<|19.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51350: AddedToken(\"<|19.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51351: AddedToken(\"<|19.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51352: AddedToken(\"<|19.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51353: AddedToken(\"<|19.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51354: AddedToken(\"<|19.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51355: AddedToken(\"<|19.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51356: AddedToken(\"<|19.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51357: AddedToken(\"<|19.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51358: AddedToken(\"<|19.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51359: AddedToken(\"<|19.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51360: AddedToken(\"<|19.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51361: AddedToken(\"<|19.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51362: AddedToken(\"<|19.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51363: AddedToken(\"<|19.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51364: AddedToken(\"<|20.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51365: AddedToken(\"<|20.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51366: AddedToken(\"<|20.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51367: AddedToken(\"<|20.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51368: AddedToken(\"<|20.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51369: AddedToken(\"<|20.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51370: AddedToken(\"<|20.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51371: AddedToken(\"<|20.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51372: AddedToken(\"<|20.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51373: AddedToken(\"<|20.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51374: AddedToken(\"<|20.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51375: AddedToken(\"<|20.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51376: AddedToken(\"<|20.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51377: AddedToken(\"<|20.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51378: AddedToken(\"<|20.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51379: AddedToken(\"<|20.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51380: AddedToken(\"<|20.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51381: AddedToken(\"<|20.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51382: AddedToken(\"<|20.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51383: AddedToken(\"<|20.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51384: AddedToken(\"<|20.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51385: AddedToken(\"<|20.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51386: AddedToken(\"<|20.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51387: AddedToken(\"<|20.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51388: AddedToken(\"<|20.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51389: AddedToken(\"<|20.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51390: AddedToken(\"<|20.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51391: AddedToken(\"<|20.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51392: AddedToken(\"<|20.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51393: AddedToken(\"<|20.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51394: AddedToken(\"<|20.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51395: AddedToken(\"<|20.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51396: AddedToken(\"<|20.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51397: AddedToken(\"<|20.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51398: AddedToken(\"<|20.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51399: AddedToken(\"<|20.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51400: AddedToken(\"<|20.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51401: AddedToken(\"<|20.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51402: AddedToken(\"<|20.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51403: AddedToken(\"<|20.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51404: AddedToken(\"<|20.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51405: AddedToken(\"<|20.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51406: AddedToken(\"<|20.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51407: AddedToken(\"<|20.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51408: AddedToken(\"<|20.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51409: AddedToken(\"<|20.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51410: AddedToken(\"<|20.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51411: AddedToken(\"<|20.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51412: AddedToken(\"<|20.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51413: AddedToken(\"<|20.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51414: AddedToken(\"<|21.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51415: AddedToken(\"<|21.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51416: AddedToken(\"<|21.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51417: AddedToken(\"<|21.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51418: AddedToken(\"<|21.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51419: AddedToken(\"<|21.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51420: AddedToken(\"<|21.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51421: AddedToken(\"<|21.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51422: AddedToken(\"<|21.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51423: AddedToken(\"<|21.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51424: AddedToken(\"<|21.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51425: AddedToken(\"<|21.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51426: AddedToken(\"<|21.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51427: AddedToken(\"<|21.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51428: AddedToken(\"<|21.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51429: AddedToken(\"<|21.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51430: AddedToken(\"<|21.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51431: AddedToken(\"<|21.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51432: AddedToken(\"<|21.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51433: AddedToken(\"<|21.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51434: AddedToken(\"<|21.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51435: AddedToken(\"<|21.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51436: AddedToken(\"<|21.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51437: AddedToken(\"<|21.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51438: AddedToken(\"<|21.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51439: AddedToken(\"<|21.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51440: AddedToken(\"<|21.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51441: AddedToken(\"<|21.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51442: AddedToken(\"<|21.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51443: AddedToken(\"<|21.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51444: AddedToken(\"<|21.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51445: AddedToken(\"<|21.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51446: AddedToken(\"<|21.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51447: AddedToken(\"<|21.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51448: AddedToken(\"<|21.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51449: AddedToken(\"<|21.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51450: AddedToken(\"<|21.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51451: AddedToken(\"<|21.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51452: AddedToken(\"<|21.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51453: AddedToken(\"<|21.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51454: AddedToken(\"<|21.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51455: AddedToken(\"<|21.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51456: AddedToken(\"<|21.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51457: AddedToken(\"<|21.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51458: AddedToken(\"<|21.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51459: AddedToken(\"<|21.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51460: AddedToken(\"<|21.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51461: AddedToken(\"<|21.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51462: AddedToken(\"<|21.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51463: AddedToken(\"<|21.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51464: AddedToken(\"<|22.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51465: AddedToken(\"<|22.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51466: AddedToken(\"<|22.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51467: AddedToken(\"<|22.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51468: AddedToken(\"<|22.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51469: AddedToken(\"<|22.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51470: AddedToken(\"<|22.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51471: AddedToken(\"<|22.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51472: AddedToken(\"<|22.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51473: AddedToken(\"<|22.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51474: AddedToken(\"<|22.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51475: AddedToken(\"<|22.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51476: AddedToken(\"<|22.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51477: AddedToken(\"<|22.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51478: AddedToken(\"<|22.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51479: AddedToken(\"<|22.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51480: AddedToken(\"<|22.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51481: AddedToken(\"<|22.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51482: AddedToken(\"<|22.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51483: AddedToken(\"<|22.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51484: AddedToken(\"<|22.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51485: AddedToken(\"<|22.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51486: AddedToken(\"<|22.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51487: AddedToken(\"<|22.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51488: AddedToken(\"<|22.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51489: AddedToken(\"<|22.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51490: AddedToken(\"<|22.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51491: AddedToken(\"<|22.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51492: AddedToken(\"<|22.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51493: AddedToken(\"<|22.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51494: AddedToken(\"<|22.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51495: AddedToken(\"<|22.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51496: AddedToken(\"<|22.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51497: AddedToken(\"<|22.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51498: AddedToken(\"<|22.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51499: AddedToken(\"<|22.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51500: AddedToken(\"<|22.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51501: AddedToken(\"<|22.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51502: AddedToken(\"<|22.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51503: AddedToken(\"<|22.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51504: AddedToken(\"<|22.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51505: AddedToken(\"<|22.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51506: AddedToken(\"<|22.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51507: AddedToken(\"<|22.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51508: AddedToken(\"<|22.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51509: AddedToken(\"<|22.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51510: AddedToken(\"<|22.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51511: AddedToken(\"<|22.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51512: AddedToken(\"<|22.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51513: AddedToken(\"<|22.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51514: AddedToken(\"<|23.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51515: AddedToken(\"<|23.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51516: AddedToken(\"<|23.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51517: AddedToken(\"<|23.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51518: AddedToken(\"<|23.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51519: AddedToken(\"<|23.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51520: AddedToken(\"<|23.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51521: AddedToken(\"<|23.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51522: AddedToken(\"<|23.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51523: AddedToken(\"<|23.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51524: AddedToken(\"<|23.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51525: AddedToken(\"<|23.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51526: AddedToken(\"<|23.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51527: AddedToken(\"<|23.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51528: AddedToken(\"<|23.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51529: AddedToken(\"<|23.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51530: AddedToken(\"<|23.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51531: AddedToken(\"<|23.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51532: AddedToken(\"<|23.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51533: AddedToken(\"<|23.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51534: AddedToken(\"<|23.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51535: AddedToken(\"<|23.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51536: AddedToken(\"<|23.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51537: AddedToken(\"<|23.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51538: AddedToken(\"<|23.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51539: AddedToken(\"<|23.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51540: AddedToken(\"<|23.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51541: AddedToken(\"<|23.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51542: AddedToken(\"<|23.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51543: AddedToken(\"<|23.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51544: AddedToken(\"<|23.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51545: AddedToken(\"<|23.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51546: AddedToken(\"<|23.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51547: AddedToken(\"<|23.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51548: AddedToken(\"<|23.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51549: AddedToken(\"<|23.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51550: AddedToken(\"<|23.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51551: AddedToken(\"<|23.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51552: AddedToken(\"<|23.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51553: AddedToken(\"<|23.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51554: AddedToken(\"<|23.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51555: AddedToken(\"<|23.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51556: AddedToken(\"<|23.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51557: AddedToken(\"<|23.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51558: AddedToken(\"<|23.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51559: AddedToken(\"<|23.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51560: AddedToken(\"<|23.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51561: AddedToken(\"<|23.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51562: AddedToken(\"<|23.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51563: AddedToken(\"<|23.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51564: AddedToken(\"<|24.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51565: AddedToken(\"<|24.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51566: AddedToken(\"<|24.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51567: AddedToken(\"<|24.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51568: AddedToken(\"<|24.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51569: AddedToken(\"<|24.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51570: AddedToken(\"<|24.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51571: AddedToken(\"<|24.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51572: AddedToken(\"<|24.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51573: AddedToken(\"<|24.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51574: AddedToken(\"<|24.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51575: AddedToken(\"<|24.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51576: AddedToken(\"<|24.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51577: AddedToken(\"<|24.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51578: AddedToken(\"<|24.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51579: AddedToken(\"<|24.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51580: AddedToken(\"<|24.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51581: AddedToken(\"<|24.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51582: AddedToken(\"<|24.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51583: AddedToken(\"<|24.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51584: AddedToken(\"<|24.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51585: AddedToken(\"<|24.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51586: AddedToken(\"<|24.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51587: AddedToken(\"<|24.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51588: AddedToken(\"<|24.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51589: AddedToken(\"<|24.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51590: AddedToken(\"<|24.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51591: AddedToken(\"<|24.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51592: AddedToken(\"<|24.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51593: AddedToken(\"<|24.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51594: AddedToken(\"<|24.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51595: AddedToken(\"<|24.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51596: AddedToken(\"<|24.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51597: AddedToken(\"<|24.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51598: AddedToken(\"<|24.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51599: AddedToken(\"<|24.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51600: AddedToken(\"<|24.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51601: AddedToken(\"<|24.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51602: AddedToken(\"<|24.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51603: AddedToken(\"<|24.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51604: AddedToken(\"<|24.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51605: AddedToken(\"<|24.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51606: AddedToken(\"<|24.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51607: AddedToken(\"<|24.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51608: AddedToken(\"<|24.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51609: AddedToken(\"<|24.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51610: AddedToken(\"<|24.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51611: AddedToken(\"<|24.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51612: AddedToken(\"<|24.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51613: AddedToken(\"<|24.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51614: AddedToken(\"<|25.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51615: AddedToken(\"<|25.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51616: AddedToken(\"<|25.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51617: AddedToken(\"<|25.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51618: AddedToken(\"<|25.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51619: AddedToken(\"<|25.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51620: AddedToken(\"<|25.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51621: AddedToken(\"<|25.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51622: AddedToken(\"<|25.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51623: AddedToken(\"<|25.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51624: AddedToken(\"<|25.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51625: AddedToken(\"<|25.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51626: AddedToken(\"<|25.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51627: AddedToken(\"<|25.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51628: AddedToken(\"<|25.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51629: AddedToken(\"<|25.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51630: AddedToken(\"<|25.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51631: AddedToken(\"<|25.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51632: AddedToken(\"<|25.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51633: AddedToken(\"<|25.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51634: AddedToken(\"<|25.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51635: AddedToken(\"<|25.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51636: AddedToken(\"<|25.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51637: AddedToken(\"<|25.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51638: AddedToken(\"<|25.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51639: AddedToken(\"<|25.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51640: AddedToken(\"<|25.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51641: AddedToken(\"<|25.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51642: AddedToken(\"<|25.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51643: AddedToken(\"<|25.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51644: AddedToken(\"<|25.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51645: AddedToken(\"<|25.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51646: AddedToken(\"<|25.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51647: AddedToken(\"<|25.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51648: AddedToken(\"<|25.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51649: AddedToken(\"<|25.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51650: AddedToken(\"<|25.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51651: AddedToken(\"<|25.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51652: AddedToken(\"<|25.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51653: AddedToken(\"<|25.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51654: AddedToken(\"<|25.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51655: AddedToken(\"<|25.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51656: AddedToken(\"<|25.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51657: AddedToken(\"<|25.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51658: AddedToken(\"<|25.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51659: AddedToken(\"<|25.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51660: AddedToken(\"<|25.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51661: AddedToken(\"<|25.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51662: AddedToken(\"<|25.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51663: AddedToken(\"<|25.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51664: AddedToken(\"<|26.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51665: AddedToken(\"<|26.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51666: AddedToken(\"<|26.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51667: AddedToken(\"<|26.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51668: AddedToken(\"<|26.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51669: AddedToken(\"<|26.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51670: AddedToken(\"<|26.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51671: AddedToken(\"<|26.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51672: AddedToken(\"<|26.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51673: AddedToken(\"<|26.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51674: AddedToken(\"<|26.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51675: AddedToken(\"<|26.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51676: AddedToken(\"<|26.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51677: AddedToken(\"<|26.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51678: AddedToken(\"<|26.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51679: AddedToken(\"<|26.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51680: AddedToken(\"<|26.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51681: AddedToken(\"<|26.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51682: AddedToken(\"<|26.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51683: AddedToken(\"<|26.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51684: AddedToken(\"<|26.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51685: AddedToken(\"<|26.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51686: AddedToken(\"<|26.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51687: AddedToken(\"<|26.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51688: AddedToken(\"<|26.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51689: AddedToken(\"<|26.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51690: AddedToken(\"<|26.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51691: AddedToken(\"<|26.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51692: AddedToken(\"<|26.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51693: AddedToken(\"<|26.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51694: AddedToken(\"<|26.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51695: AddedToken(\"<|26.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51696: AddedToken(\"<|26.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51697: AddedToken(\"<|26.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51698: AddedToken(\"<|26.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51699: AddedToken(\"<|26.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51700: AddedToken(\"<|26.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51701: AddedToken(\"<|26.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51702: AddedToken(\"<|26.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51703: AddedToken(\"<|26.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51704: AddedToken(\"<|26.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51705: AddedToken(\"<|26.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51706: AddedToken(\"<|26.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51707: AddedToken(\"<|26.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51708: AddedToken(\"<|26.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51709: AddedToken(\"<|26.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51710: AddedToken(\"<|26.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51711: AddedToken(\"<|26.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51712: AddedToken(\"<|26.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51713: AddedToken(\"<|26.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51714: AddedToken(\"<|27.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51715: AddedToken(\"<|27.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51716: AddedToken(\"<|27.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51717: AddedToken(\"<|27.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51718: AddedToken(\"<|27.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51719: AddedToken(\"<|27.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51720: AddedToken(\"<|27.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51721: AddedToken(\"<|27.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51722: AddedToken(\"<|27.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51723: AddedToken(\"<|27.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51724: AddedToken(\"<|27.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51725: AddedToken(\"<|27.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51726: AddedToken(\"<|27.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51727: AddedToken(\"<|27.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51728: AddedToken(\"<|27.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51729: AddedToken(\"<|27.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51730: AddedToken(\"<|27.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51731: AddedToken(\"<|27.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51732: AddedToken(\"<|27.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51733: AddedToken(\"<|27.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51734: AddedToken(\"<|27.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51735: AddedToken(\"<|27.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51736: AddedToken(\"<|27.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51737: AddedToken(\"<|27.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51738: AddedToken(\"<|27.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51739: AddedToken(\"<|27.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51740: AddedToken(\"<|27.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51741: AddedToken(\"<|27.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51742: AddedToken(\"<|27.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51743: AddedToken(\"<|27.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51744: AddedToken(\"<|27.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51745: AddedToken(\"<|27.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51746: AddedToken(\"<|27.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51747: AddedToken(\"<|27.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51748: AddedToken(\"<|27.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51749: AddedToken(\"<|27.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51750: AddedToken(\"<|27.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51751: AddedToken(\"<|27.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51752: AddedToken(\"<|27.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51753: AddedToken(\"<|27.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51754: AddedToken(\"<|27.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51755: AddedToken(\"<|27.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51756: AddedToken(\"<|27.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51757: AddedToken(\"<|27.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51758: AddedToken(\"<|27.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51759: AddedToken(\"<|27.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51760: AddedToken(\"<|27.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51761: AddedToken(\"<|27.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51762: AddedToken(\"<|27.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51763: AddedToken(\"<|27.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51764: AddedToken(\"<|28.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51765: AddedToken(\"<|28.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51766: AddedToken(\"<|28.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51767: AddedToken(\"<|28.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51768: AddedToken(\"<|28.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51769: AddedToken(\"<|28.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51770: AddedToken(\"<|28.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51771: AddedToken(\"<|28.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51772: AddedToken(\"<|28.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51773: AddedToken(\"<|28.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51774: AddedToken(\"<|28.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51775: AddedToken(\"<|28.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51776: AddedToken(\"<|28.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51777: AddedToken(\"<|28.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51778: AddedToken(\"<|28.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51779: AddedToken(\"<|28.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51780: AddedToken(\"<|28.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51781: AddedToken(\"<|28.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51782: AddedToken(\"<|28.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51783: AddedToken(\"<|28.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51784: AddedToken(\"<|28.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51785: AddedToken(\"<|28.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51786: AddedToken(\"<|28.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51787: AddedToken(\"<|28.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51788: AddedToken(\"<|28.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51789: AddedToken(\"<|28.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51790: AddedToken(\"<|28.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51791: AddedToken(\"<|28.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51792: AddedToken(\"<|28.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51793: AddedToken(\"<|28.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51794: AddedToken(\"<|28.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51795: AddedToken(\"<|28.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51796: AddedToken(\"<|28.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51797: AddedToken(\"<|28.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51798: AddedToken(\"<|28.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51799: AddedToken(\"<|28.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51800: AddedToken(\"<|28.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51801: AddedToken(\"<|28.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51802: AddedToken(\"<|28.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51803: AddedToken(\"<|28.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51804: AddedToken(\"<|28.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51805: AddedToken(\"<|28.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51806: AddedToken(\"<|28.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51807: AddedToken(\"<|28.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51808: AddedToken(\"<|28.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51809: AddedToken(\"<|28.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51810: AddedToken(\"<|28.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51811: AddedToken(\"<|28.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51812: AddedToken(\"<|28.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51813: AddedToken(\"<|28.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51814: AddedToken(\"<|29.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51815: AddedToken(\"<|29.02|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51816: AddedToken(\"<|29.04|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51817: AddedToken(\"<|29.06|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51818: AddedToken(\"<|29.08|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51819: AddedToken(\"<|29.10|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51820: AddedToken(\"<|29.12|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51821: AddedToken(\"<|29.14|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51822: AddedToken(\"<|29.16|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51823: AddedToken(\"<|29.18|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51824: AddedToken(\"<|29.20|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51825: AddedToken(\"<|29.22|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51826: AddedToken(\"<|29.24|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51827: AddedToken(\"<|29.26|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51828: AddedToken(\"<|29.28|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51829: AddedToken(\"<|29.30|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51830: AddedToken(\"<|29.32|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51831: AddedToken(\"<|29.34|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51832: AddedToken(\"<|29.36|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51833: AddedToken(\"<|29.38|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51834: AddedToken(\"<|29.40|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51835: AddedToken(\"<|29.42|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51836: AddedToken(\"<|29.44|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51837: AddedToken(\"<|29.46|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51838: AddedToken(\"<|29.48|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51839: AddedToken(\"<|29.50|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51840: AddedToken(\"<|29.52|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51841: AddedToken(\"<|29.54|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51842: AddedToken(\"<|29.56|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51843: AddedToken(\"<|29.58|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51844: AddedToken(\"<|29.60|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51845: AddedToken(\"<|29.62|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51846: AddedToken(\"<|29.64|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51847: AddedToken(\"<|29.66|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51848: AddedToken(\"<|29.68|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51849: AddedToken(\"<|29.70|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51850: AddedToken(\"<|29.72|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51851: AddedToken(\"<|29.74|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51852: AddedToken(\"<|29.76|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51853: AddedToken(\"<|29.78|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51854: AddedToken(\"<|29.80|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51855: AddedToken(\"<|29.82|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51856: AddedToken(\"<|29.84|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51857: AddedToken(\"<|29.86|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51858: AddedToken(\"<|29.88|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51859: AddedToken(\"<|29.90|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51860: AddedToken(\"<|29.92|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51861: AddedToken(\"<|29.94|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51862: AddedToken(\"<|29.96|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51863: AddedToken(\"<|29.98|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51864: AddedToken(\"<|30.00|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"WhisperProcessor\"\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-small\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/model.safetensors\n",
      "Instantiating WhisperForConditionalGeneration model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--openai--whisper-small/snapshots/973afd24965f72e36ca33b3055d56a652f456b4d/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      5,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      9\n",
      "    ],\n",
      "    [\n",
      "      8,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      8,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      8,\n",
      "      7\n",
      "    ],\n",
      "    [\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      9,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      9,\n",
      "      7\n",
      "    ],\n",
      "    [\n",
      "      9,\n",
      "      9\n",
      "    ],\n",
      "    [\n",
      "      10,\n",
      "      5\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터: 3,538,944 / 전체 파라미터: 245,273,856\n",
      "학습 가능한 파라미터 비율: 1.44%\n",
      "\n",
      "학습 가능한 파라미터 목록:\n",
      "- base_model.model.model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.0.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.1.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.2.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.3.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.4.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.5.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.6.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.7.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.8.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.9.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.10.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.self_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.k_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.k_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.out_proj.lora_A.default.weight: torch.Size([16, 768])\n",
      "- base_model.model.model.decoder.layers.11.encoder_attn.out_proj.lora_B.default.weight: torch.Size([768, 16])\n",
      "trainable params: 3,538,944 || all params: 245,273,856 || trainable%: 1.4429\n",
      "\n",
      "모델 학습 모드: True\n",
      "학습 가능한 파라미터 수: 288\n",
      "데이터셋 처리 시작...\n",
      "훈련 데이터셋 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train chunk 1: 100%|██████████████████████████████| 1000/1000 [00:10<00:00, 92.74 examples/s]\n",
      "Processing train chunk 2: 100%|██████████████████████████████| 1000/1000 [00:14<00:00, 70.58 examples/s]\n",
      "Processing train chunk 3: 100%|██████████████████████████████| 1000/1000 [00:14<00:00, 68.71 examples/s]\n",
      "Processing train chunk 4: 100%|█████████████████████████████| 1000/1000 [00:08<00:00, 121.65 examples/s]\n",
      "Processing train chunk 5: 100%|█████████████████████████████| 1000/1000 [00:08<00:00, 119.28 examples/s]\n",
      "Processing train chunk 6: 100%|█████████████████████████████| 1000/1000 [00:08<00:00, 124.31 examples/s]\n",
      "Processing train chunk 7: 100%|█████████████████████████████| 1000/1000 [00:07<00:00, 126.65 examples/s]\n",
      "Processing train chunk 8: 100%|█████████████████████████████| 1000/1000 [00:08<00:00, 123.58 examples/s]\n",
      "Processing train chunk 9: 100%|█████████████████████████████| 1000/1000 [00:08<00:00, 111.43 examples/s]\n",
      "Processing train chunk 10: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 115.29 examples/s]\n",
      "Processing train chunk 11: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 118.95 examples/s]\n",
      "Processing train chunk 12: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 119.12 examples/s]\n",
      "Processing train chunk 13: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 117.13 examples/s]\n",
      "Processing train chunk 14: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 117.53 examples/s]\n",
      "Processing train chunk 15: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 118.83 examples/s]\n",
      "Processing train chunk 16: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 119.17 examples/s]\n",
      "Processing train chunk 17: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 118.49 examples/s]\n",
      "Processing train chunk 18: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 116.08 examples/s]\n",
      "Processing train chunk 19: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 117.24 examples/s]\n",
      "Processing train chunk 20: 100%|████████████████████████████| 1000/1000 [00:08<00:00, 117.79 examples/s]\n",
      "Processing train chunk 21: 100%|██████████████████████████████| 623/623 [00:05<00:00, 108.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터셋 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid chunk 1: 100%|████████████████████████████████| 576/576 [00:06<00:00, 92.97 examples/s]\n",
      "Saving the dataset (40/40 shards): 100%|██████████████████| 20623/20623 [00:30<00:00, 678.50 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|████████████████████████| 576/576 [00:00<00:00, 862.44 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터셋 저장 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Optimizer is None, initializing default optimizer...\n",
      "\n",
      "최종 모델 상태:\n",
      "학습 모드: True\n",
      "학습 가능한 파라미터 수: 288\n",
      "Optimizer: AdamW\n",
      "Learning rate: 0.0\n",
      "체크포인트가 없어 처음부터 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 20,623\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 4,000\n",
      "  Number of trainable parameters = 3,538,944\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9822/3578381076.py\", line 635, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2123, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 433, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py\", line 820, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_9822/3578381076.py\", line 702, in forward\n",
      "    with self._enable_peft_forward_hooks(**kwargs):\n",
      "  File \"/tmp/ipykernel_9822/3578381076.py\", line 707, in torch_dynamo_resume_in_forward_at_702\n",
      "    outputs = self.base_model(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1767, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 1116, in __call__\n",
      "    return self._torchdynamo_orig_callable(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\n",
      "    result = self._inner_convert(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\n",
      "    return _compile(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_utils_internal.py\", line 84, in wrapper_function\n",
      "    return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\n",
      "    super().run()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 499, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1459, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\", line 743, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/misc.py\", line 636, in call_function\n",
      "    return self.fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/user_defined.py\", line 202, in fake_cross_entropy_loss\n",
      "    return wrap_fx_proxy(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py\", line 1713, in wrap_fx_proxy\n",
      "    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py\", line 1798, in wrap_fx_proxy_cls\n",
      "    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1853, in get_fake_value\n",
      "    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1785, in get_fake_value\n",
      "    ret_val = wrap_fake_exception(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1300, in wrap_fake_exception\n",
      "    return fn()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1786, in <lambda>\n",
      "    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1921, in run_node\n",
      "    raise RuntimeError(make_error_message(e)).with_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\", line 1903, in run_node\n",
      "    return node.target(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch._dynamo.exc.TorchRuntimeError: Failed running call_function <function cross_entropy at 0x7f053cd6c790>(*(FakeTensor(..., device='cuda:0', size=(800, 51865), dtype=torch.float16), FakeTensor(..., device='cuda:0', size=(784,), dtype=torch.int64), None, None, -100, None, 'mean', 0.0), **{}):\n",
      "Expected input batch_size (800) to match target batch_size (784).\n",
      "\n",
      "from user code:\n",
      "   File \"/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1792, in torch_dynamo_resume_in_forward_at_1767\n",
      "    loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.reshape(-1))\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "Failed running call_function <function cross_entropy at 0x7f053cd6c790>(*(FakeTensor(..., device='cuda:0', size=(800, 51865), dtype=torch.float16), FakeTensor(..., device='cuda:0', size=(784,), dtype=torch.int64), None, None, -100, None, 'mean', 0.0), **{}):\nExpected input batch_size (800) to match target batch_size (784).\n\nfrom user code:\n   File \"/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1792, in torch_dynamo_resume_in_forward_at_1767\n    loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.reshape(-1))\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 738\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    737\u001b[0m     optimize_memory()\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[4], line 635\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m체크포인트가 없어 처음부터 시작합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 635\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m출력 디렉토리가 없어 처음부터 시작합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 702\u001b[0m, in \u001b[0;36mWhisperPEFTModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m             decoder_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([decoder_input_ids, padding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# PEFT 모델의 forward 메서드 호출\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    703\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \n\u001b[1;32m    704\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args \n\u001b[1;32m    705\u001b[0m              \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforced_decoder_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    707\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m    708\u001b[0m         input_features\u001b[38;5;241m=\u001b[39minput_features,\n\u001b[1;32m    709\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    717\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 707\u001b[0m, in \u001b[0;36mtorch_dynamo_resume_in_forward_at_702\u001b[0;34m(___stack0, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    703\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \n\u001b[1;32m    704\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args \n\u001b[1;32m    705\u001b[0m              \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforced_decoder_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 707\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m    708\u001b[0m         input_features\u001b[38;5;241m=\u001b[39minput_features,\n\u001b[1;32m    709\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    710\u001b[0m         decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m    711\u001b[0m         decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m    712\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    714\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    715\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# 손실 계산 전 배치 크기 확인 및 조정\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py:1767\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1763\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1764\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1765\u001b[0m         )\n\u001b[0;32m-> 1767\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1768\u001b[0m     input_features,\n\u001b[1;32m   1769\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1770\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1771\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[1;32m   1772\u001b[0m     decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1773\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1774\u001b[0m     decoder_head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1775\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1776\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1777\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1778\u001b[0m     decoder_position_ids\u001b[38;5;241m=\u001b[39mdecoder_position_ids,\n\u001b[1;32m   1779\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1780\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1781\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1782\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1783\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1784\u001b[0m )\n\u001b[1;32m   1785\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1787\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1112\u001b[0m             )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:948\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    946\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    460\u001b[0m signpost_event(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     },\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_strobelight/compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[0;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     Unsupported,\n\u001b[1;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     BisectValidationException,\n\u001b[1;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:1459\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1457\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1458\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/misc.py:636\u001b[0m, in \u001b[0;36mLambdaVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/user_defined.py:202\u001b[0m, in \u001b[0;36mUserDefinedClassVariable._call_cross_entropy_loss.<locals>.fake_cross_entropy_loss\u001b[0;34m(input, target)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfake_cross_entropy_loss\u001b[39m(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_fx_proxy\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproxy_args_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreduce_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py:1713\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1705\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m: tx,\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m   1711\u001b[0m }\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorVariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrap_fx_proxy_cls(target_cls\u001b[38;5;241m=\u001b[39mTensorWithTFOverrideVariable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py:1798\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# with preserve_rng_state():\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[0;32m-> 1798\u001b[0m     example_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_fake_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_graph_fake\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;66;03m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m maybe_get_fake_mode(example_value) \u001b[38;5;129;01mis\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1853\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(cause):\n\u001b[1;32m   1851\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypeError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcause\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1853\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[1;32m   1856\u001b[0m     _ \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m   1857\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, functools\u001b[38;5;241m.\u001b[39mpartial(ensure_graph_fake, tx\u001b[38;5;241m=\u001b[39mtx), ret_val\n\u001b[1;32m   1858\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1785\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 1785\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_fake_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1300\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_fake_exception\u001b[39m(fn):\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unimplemented\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1786\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   1785\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m wrap_fake_exception(\n\u001b[0;32m-> 1786\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1787\u001b[0m         )\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1921\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1919\u001b[0m         unimplemented(make_error_message(e), from_exc\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e))\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1922\u001b[0m             e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1923\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py:1903\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], node\u001b[38;5;241m.\u001b[39mtarget)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m: Failed running call_function <function cross_entropy at 0x7f053cd6c790>(*(FakeTensor(..., device='cuda:0', size=(800, 51865), dtype=torch.float16), FakeTensor(..., device='cuda:0', size=(784,), dtype=torch.int64), None, None, -100, None, 'mean', 0.0), **{}):\nExpected input batch_size (800) to match target batch_size (784).\n\nfrom user code:\n   File \"/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1792, in torch_dynamo_resume_in_forward_at_1767\n    loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.reshape(-1))\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import logging\n",
    "import evaluate\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "# Hugging Face 라이브러리\n",
    "from datasets import Dataset, DatasetDict, Audio, Features, Array2D, Sequence, Value, concatenate_datasets\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModelForSeq2SeqLM\n",
    "\n",
    "# 로깅 설정\n",
    "logging.getLogger(\"transformers\").setLevel(logging.INFO)\n",
    "\n",
    "# 환경 설정\n",
    "output_dir = \"./whisper-korean-ft2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"cache\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"temp\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"test_samples\"), exist_ok=True)\n",
    "\n",
    "# 데이터셋 설정\n",
    "VALID_CSV_FILE = 'filtered_data_val.csv'\n",
    "TRAIN_AUDIO_DIR = 'train'\n",
    "VALID_AUDIO_DIR = 'valid'\n",
    "TRAIN_CSV_FILES = ['filtered_data_A.csv', 'filtered_data_B.csv']\n",
    "\n",
    "# 1. 데이터 콜레이터\n",
    "@dataclass\n",
    "class WhisperDataCollator:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # 배치 크기 확인\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # 입력 특징 처리\n",
    "        input_features = [feature[\"input_features\"] for feature in features]\n",
    "        input_features = torch.tensor(np.array(input_features), dtype=torch.float32)\n",
    "        \n",
    "        # 레이블 처리\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        \n",
    "        # 디코더 입력 ID 생성\n",
    "        decoder_input_ids = []\n",
    "        for label_ids in labels:\n",
    "            decoder_input_ids.append([self.processor.tokenizer.bos_token_id] + label_ids.tolist())\n",
    "        \n",
    "        # 패딩 적용\n",
    "        max_length = max(len(ids) for ids in decoder_input_ids)\n",
    "        decoder_input_ids = [\n",
    "            ids + [self.processor.tokenizer.pad_token_id] * (max_length - len(ids))\n",
    "            for ids in decoder_input_ids\n",
    "        ]\n",
    "        decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "        \n",
    "        # 배치 구성\n",
    "        batch = {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels,\n",
    "            \"decoder_input_ids\": decoder_input_ids\n",
    "        }\n",
    "        \n",
    "        # 모든 텐서의 배치 크기 확인 및 조정\n",
    "        for key, tensor in batch.items():\n",
    "            if tensor.size(0) != batch_size:\n",
    "                print(f\"Warning: {key} batch size mismatch. Expected {batch_size}, got {tensor.size(0)}\")\n",
    "                if tensor.size(0) > batch_size:\n",
    "                    batch[key] = tensor[:batch_size]\n",
    "                else:\n",
    "                    padding = torch.zeros((batch_size - tensor.size(0), *tensor.size()[1:]), \n",
    "                                       dtype=tensor.dtype, \n",
    "                                       device=tensor.device)\n",
    "                    batch[key] = torch.cat([tensor, padding], dim=0)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# 2. 메모리 모니터링 콜백\n",
    "class MemoryMonitorCallback(TrainerCallback):\n",
    "    def __init__(self, threshold_gb=20):\n",
    "        self.threshold_gb = threshold_gb\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            memory_allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "            if memory_allocated > self.threshold_gb:\n",
    "                print(f\"경고: GPU 메모리 사용량 {memory_allocated:.2f}GB\")\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"메모리 최적화 완료\")\n",
    "        return control\n",
    "\n",
    "# 3. 시간 기반 체크포인트\n",
    "class TimeCheckpoint(TrainerCallback):\n",
    "    def __init__(self, interval=30):\n",
    "        self.interval = timedelta(minutes=interval)\n",
    "        self.last_save = datetime.now()\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, model=None, trainer=None, **kwargs):\n",
    "        self.model = model\n",
    "        self.trainer = trainer\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if datetime.now() - self.last_save >= self.interval:\n",
    "            checkpoint_dir = os.path.join(\n",
    "                args.output_dir, \n",
    "                f\"checkpoint-time-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "            )\n",
    "            self.model.save_pretrained(checkpoint_dir)\n",
    "            if state is not None:\n",
    "                state.save_to_json(os.path.join(checkpoint_dir, \"trainer_state.json\"))\n",
    "            self.last_save = datetime.now()\n",
    "            print(f\"시간 기반 체크포인트 저장: {checkpoint_dir}\")\n",
    "        return control\n",
    "\n",
    "# 4. 에러 로거\n",
    "class ErrorLogger:\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "    def log(self, message):\n",
    "        with open(self.log_path, \"a\") as f:\n",
    "            f.write(f\"{datetime.now().isoformat()} - {message}\\n\")\n",
    "\n",
    "# 5. 오디오 처리\n",
    "class AudioProcessor:\n",
    "    def __init__(self, processor, max_seconds=30):\n",
    "        self.processor = processor\n",
    "        self.max_seconds = max_seconds\n",
    "        \n",
    "    def process_audio(self, audio):\n",
    "        \"\"\"외국인 발화 특성 반영 전처리\"\"\"\n",
    "        try:\n",
    "            if isinstance(audio, dict) and \"array\" in audio:\n",
    "                array = audio[\"array\"]\n",
    "                sr = audio.get(\"sampling_rate\", 16000)\n",
    "            else:\n",
    "                raise ValueError(\"잘못된 오디오 형식\")\n",
    "            \n",
    "            array = self.adaptive_normalize(array)\n",
    "            max_samples = int(self.max_seconds * sr * 1.2)\n",
    "            if len(array) > max_samples:\n",
    "                array = array[:max_samples]\n",
    "            \n",
    "            feature = self.processor(\n",
    "                array, \n",
    "                sampling_rate=sr, \n",
    "                return_tensors=\"np\", \n",
    "                truncation=False\n",
    "            ).input_features[0]\n",
    "            \n",
    "            if feature.shape != (80, 3000):\n",
    "                fixed = np.zeros((80, 3000), dtype=feature.dtype)\n",
    "                h, w = feature.shape\n",
    "                fixed[:h, :w] = feature[:min(h,80), :min(w,3000)]\n",
    "                feature = fixed\n",
    "            return feature.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오디오 처리 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def adaptive_normalize(self, waveform, target_level=-16.0, frame_length_ms=500, max_gain_db=30.0):\n",
    "        if len(waveform) == 0:\n",
    "            return waveform\n",
    "            \n",
    "        sample_rate = 16000\n",
    "        frame_length = int(sample_rate * frame_length_ms / 1000)\n",
    "        num_frames = max(1, len(waveform) // frame_length)\n",
    "        normalized = np.zeros_like(waveform)\n",
    "        \n",
    "        for i in range(num_frames):\n",
    "            start = i * frame_length\n",
    "            end = min(start + frame_length, len(waveform))\n",
    "            frame = waveform[start:end]\n",
    "            rms = np.sqrt(np.mean(frame**2))\n",
    "            \n",
    "            if rms < 1e-8:\n",
    "                normalized[start:end] = frame\n",
    "                continue\n",
    "                \n",
    "            current_level = 20 * np.log10(rms) if rms > 0 else -100\n",
    "            gain_db = target_level - current_level\n",
    "            gain_db = min(max_gain_db, gain_db)\n",
    "            gain_linear = 10 ** (gain_db / 20)\n",
    "            normalized[start:end] = frame * gain_linear\n",
    "        \n",
    "        if np.max(np.abs(normalized)) > 0.99:\n",
    "            normalized = normalized / np.max(np.abs(normalized)) * 0.99\n",
    "            \n",
    "        return normalized\n",
    "\n",
    "    def split_audio(self, array, sr):\n",
    "        max_samples = int(self.max_seconds * sr)\n",
    "        return [array[i:i+max_samples] for i in range(0, len(array), max_samples)]\n",
    "\n",
    "# 6. 데이터셋 로드 함수\n",
    "def load_dataset_with_fallback():\n",
    "    try:\n",
    "        cache_path = os.path.join(output_dir, \"processed_dataset\")\n",
    "        if os.path.exists(cache_path) and os.path.isfile(os.path.join(cache_path, \"dataset_dict.json\")):\n",
    "            print(\"캐시된 데이터셋 사용\")\n",
    "            dataset_dict = DatasetDict.load_from_disk(cache_path)\n",
    "            return dataset_dict, False\n",
    "            \n",
    "        print(\"CSV 파일에서 데이터셋 생성 중...\")\n",
    "        \n",
    "        train_dfs = []\n",
    "        for csv_file in TRAIN_CSV_FILES:\n",
    "            print(f\"훈련 CSV 파일 로드 중: {csv_file}\")\n",
    "            df = pd.read_csv(csv_file)\n",
    "            train_dfs.append(df)\n",
    "        \n",
    "        train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "        print(f\"총 {len(train_df)} 개의 훈련 샘플 로드됨\")\n",
    "        \n",
    "        print(f\"검증 CSV 파일 로드 중: {VALID_CSV_FILE}\")\n",
    "        valid_df = pd.read_csv(VALID_CSV_FILE)\n",
    "        print(f\"총 {len(valid_df)} 개의 검증 샘플 로드됨\")\n",
    "        \n",
    "        train_df['audio'] = train_df['fileName'].apply(lambda fn: os.path.join(TRAIN_AUDIO_DIR, fn))\n",
    "        valid_df['audio'] = valid_df['fileName'].apply(lambda fn: os.path.join(VALID_AUDIO_DIR, fn))\n",
    "        \n",
    "        train_df = train_df[['audio', 'ReadingLabelText']]\n",
    "        valid_df = valid_df[['audio', 'ReadingLabelText']]\n",
    "        \n",
    "        train_df = train_df.rename(columns={'ReadingLabelText': 'transcripts'})\n",
    "        valid_df = valid_df.rename(columns={'ReadingLabelText': 'transcripts'})\n",
    "        \n",
    "        train_df = train_df.dropna()\n",
    "        valid_df = valid_df.dropna()\n",
    "        print(f\"결측치 제거 후 훈련 {len(train_df)}개, 검증 {len(valid_df)}개 샘플 남음\")\n",
    "        \n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        valid_dataset = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
    "        valid_dataset = valid_dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
    "        \n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'valid': valid_dataset\n",
    "        })\n",
    "        \n",
    "        dataset_dict.save_to_disk(cache_path)\n",
    "        print(f\"데이터셋 처리 완료 및 저장됨: {cache_path}\")\n",
    "        \n",
    "        return dataset_dict, False\n",
    "    except Exception as e:\n",
    "        print(f\"데이터셋 로드 실패: {e}\")\n",
    "        raise\n",
    "\n",
    "# 7. 모델 설정\n",
    "def setup_model():\n",
    "    model_id = \"openai/whisper-small\"\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        language=\"korean\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Whisper 모델의 입력 형식에 맞게 설정\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# 8. LoRA 설정\n",
    "def get_lora_config():\n",
    "    \"\"\"LoRA 설정 (외국인 발화 최적화)\"\"\"\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\"\n",
    "    )\n",
    "\n",
    "# 9. 메트릭 계산\n",
    "def compute_metrics(processor):\n",
    "    cer_metric = evaluate.load(\"cer\")\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    def metrics_fn(pred):\n",
    "        pred_str = processor.batch_decode(pred.predictions, skip_special_tokens=True)\n",
    "        label_str = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "        return {\n",
    "            \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n",
    "            \"wer\": wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        }\n",
    "    return metrics_fn\n",
    "\n",
    "# 10. 추론 함수\n",
    "def transcribe_audio(model, processor, audio_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000)\n",
    "        input_features = processor(\n",
    "            audio, \n",
    "            sampling_rate=sr, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(model.device)\n",
    "        \n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"language\": \"ko\",\n",
    "            \"task\": \"transcribe\",\n",
    "            \"num_beams\": 5,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features, **gen_kwargs)\n",
    "        \n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"추론 오류: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def transcribe_long_audio(model, processor, audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    chunks = AudioProcessor(processor).split_audio(audio, sr)\n",
    "    \n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_path = os.path.join(output_dir, \"temp\", f\"temp_{i}.wav\")\n",
    "        sf.write(chunk_path, chunk, sr)\n",
    "        result = transcribe_audio(model, processor, chunk_path)\n",
    "        results.append(result)\n",
    "        os.remove(chunk_path)\n",
    "    \n",
    "    return \" \".join(results)\n",
    "\n",
    "# 11. 검증 함수\n",
    "def validate_model(model_dir):\n",
    "    \"\"\"훈련된 모델 단독 검증\"\"\"\n",
    "    try:\n",
    "        print(f\"모델 로드 중: {model_dir}\")\n",
    "        processor = WhisperProcessor.from_pretrained(model_dir)\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            model_dir,\n",
    "            torch_dtype=torch.float32,\n",
    "        )\n",
    "        \n",
    "        test_dir = os.path.join(output_dir, \"test_samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "        \n",
    "        test_files = []\n",
    "        for file in os.listdir(test_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                test_files.append((os.path.join(test_dir, file), file))\n",
    "        \n",
    "        if not test_files:\n",
    "            print(f\"경고: 테스트 파일이 없습니다. {test_dir} 디렉토리에 오디오 파일을 추가하세요.\")\n",
    "            print(\"기본 테스트 파일을 생성합니다...\")\n",
    "            sample_audio = np.zeros(16000)\n",
    "            sample_path = os.path.join(test_dir, \"sample.wav\")\n",
    "            sf.write(sample_path, sample_audio, 16000)\n",
    "            test_files = [(sample_path, \"sample.wav\")]\n",
    "            print(f\"기본 테스트 파일 생성됨: {sample_path}\")\n",
    "        \n",
    "        print(\"\\n===== 모델 테스트 =====\")\n",
    "        print(f\"테스트 파일 수: {len(test_files)}\")\n",
    "        \n",
    "        for file_path, file_name in test_files:\n",
    "            print(f\"\\n[테스트] {file_name}\")\n",
    "            start_time = datetime.now()\n",
    "            result = transcribe_audio(model, processor, file_path)\n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"결과 ({elapsed:.2f}초):\")\n",
    "            print(f\"  {result[:100]}...\" if len(result) > 100 else result)\n",
    "        \n",
    "        print(\"\\n검증 완료!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"검증 중 오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 12. 메인 훈련 함수\n",
    "def train():\n",
    "    \"\"\"메인 훈련 함수\"\"\"\n",
    "    error_logger = ErrorLogger(os.path.join(output_dir, \"error_log.txt\"))\n",
    "    \n",
    "    try:\n",
    "        dataset, is_streaming = load_dataset_with_fallback()\n",
    "        audio_field, text_field = \"audio\", \"transcripts\"\n",
    "        \n",
    "        model, processor = setup_model()\n",
    "        \n",
    "        # 모델을 학습 모드로 설정\n",
    "        model.train()\n",
    "        \n",
    "        # LoRA 설정 및 적용\n",
    "        lora_config = get_lora_config()\n",
    "        \n",
    "        # WhisperPEFTModel로 직접 초기화\n",
    "        model = WhisperPEFTModel(model, lora_config)\n",
    "        model.processor = processor\n",
    "        \n",
    "        # 학습 가능한 파라미터 설정\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(x in name for x in [\"lora\", \"adapter\"]):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 학습 가능한 파라미터 출력\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"학습 가능한 파라미터: {trainable_params:,} / 전체 파라미터: {all_params:,}\")\n",
    "        print(f\"학습 가능한 파라미터 비율: {100 * trainable_params / all_params:.2f}%\")\n",
    "        \n",
    "        # 학습 가능한 파라미터 이름 출력\n",
    "        print(\"\\n학습 가능한 파라미터 목록:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"- {name}: {param.shape}\")\n",
    "        \n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        # 모델이 학습 모드인지 확인\n",
    "        model.train()\n",
    "        print(\"\\n모델 학습 모드:\", model.training)\n",
    "        print(\"학습 가능한 파라미터 수:\", sum(p.requires_grad for p in model.parameters()))\n",
    "        \n",
    "        audio_processor = AudioProcessor(processor)\n",
    "        \n",
    "        def process_batch(batch):\n",
    "            features = []\n",
    "            labels = []\n",
    "            \n",
    "            for audio, text in zip(batch[audio_field], batch[text_field]):\n",
    "                try:\n",
    "                    feature = audio_processor.process_audio(audio)\n",
    "                    features.append(feature)\n",
    "                    label = processor.tokenizer(text).input_ids\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"배치 처리 오류: {e}\")\n",
    "                    raise\n",
    "            \n",
    "            # 배치 크기 확인 및 조정\n",
    "            if len(features) != len(labels):\n",
    "                print(f\"Warning: Features length ({len(features)}) != Labels length ({len(labels)})\")\n",
    "                min_length = min(len(features), len(labels))\n",
    "                features = features[:min_length]\n",
    "                labels = labels[:min_length]\n",
    "            \n",
    "            # 입력 특징을 numpy 배열로 변환\n",
    "            features = np.array(features)\n",
    "            \n",
    "            return {\n",
    "                \"input_features\": features,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "        \n",
    "        print(\"데이터셋 처리 시작...\")\n",
    "        if is_streaming:\n",
    "            remove_columns = list(next(iter(dataset[\"train\"])).keys())\n",
    "            if \"__index_level_0__\" not in remove_columns:\n",
    "                remove_columns.append(\"__index_level_0__\")\n",
    "            processed_dataset = dataset.map(\n",
    "                process_batch,\n",
    "                batched=True,\n",
    "                batch_size=16,\n",
    "                remove_columns=remove_columns\n",
    "            )\n",
    "        else:\n",
    "            features = Features({\n",
    "                'input_features': Array2D(shape=(80, 3000), dtype='float32'),\n",
    "                'labels': Sequence(feature=Value(dtype='int64'))\n",
    "            })\n",
    "            \n",
    "            remove_columns = [col for col in dataset[\"train\"].column_names if col not in (\"input_features\", \"labels\")]\n",
    "            \n",
    "            # 데이터셋을 더 작은 청크로 나누어 처리\n",
    "            chunk_size = 1000  # 한 번에 처리할 샘플 수\n",
    "            train_chunks = []\n",
    "            valid_chunks = []\n",
    "            \n",
    "            print(\"훈련 데이터셋 처리 중...\")\n",
    "            for i in range(0, len(dataset[\"train\"]), chunk_size):\n",
    "                chunk = dataset[\"train\"].select(range(i, min(i + chunk_size, len(dataset[\"train\"]))))\n",
    "                processed_chunk = chunk.map(\n",
    "                    process_batch,\n",
    "                    batched=True,\n",
    "                    batch_size=16,\n",
    "                    num_proc=1,  # 병렬 처리 비활성화\n",
    "                    features=features,\n",
    "                    remove_columns=remove_columns,\n",
    "                    desc=f\"Processing train chunk {i//chunk_size + 1}\"\n",
    "                )\n",
    "                train_chunks.append(processed_chunk)\n",
    "            \n",
    "            print(\"검증 데이터셋 처리 중...\")\n",
    "            for i in range(0, len(dataset[\"valid\"]), chunk_size):\n",
    "                chunk = dataset[\"valid\"].select(range(i, min(i + chunk_size, len(dataset[\"valid\"]))))\n",
    "                processed_chunk = chunk.map(\n",
    "                    process_batch,\n",
    "                    batched=True,\n",
    "                    batch_size=16,\n",
    "                    num_proc=1,  # 병렬 처리 비활성화\n",
    "                    features=features,\n",
    "                    remove_columns=remove_columns,\n",
    "                    desc=f\"Processing valid chunk {i//chunk_size + 1}\"\n",
    "                )\n",
    "                valid_chunks.append(processed_chunk)\n",
    "            \n",
    "            # 청크 합치기\n",
    "            processed_dataset = DatasetDict({\n",
    "                \"train\": concatenate_datasets(train_chunks),\n",
    "                \"valid\": concatenate_datasets(valid_chunks)\n",
    "            })\n",
    "            \n",
    "            # 캐시 저장\n",
    "            processed_dataset.save_to_disk(os.path.join(output_dir, \"processed_dataset\"))\n",
    "            print(\"처리된 데이터셋 저장 완료\")\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=5e-5,\n",
    "            max_steps=4000,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            report_to=[\"tensorboard\"],\n",
    "            metric_for_best_model=\"eval_cer\",\n",
    "            greater_is_better=False,\n",
    "            logging_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            ddp_find_unused_parameters=False,\n",
    "            tf32=True,\n",
    "            warmup_steps=200,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            save_steps=200,\n",
    "            dataloader_num_workers=2,  # 4에서 2로 감소\n",
    "            dataloader_pin_memory=True,\n",
    "            torch_compile=True,\n",
    "        )\n",
    "        \n",
    "        # 콜백 설정\n",
    "        callbacks = [\n",
    "            TimeCheckpoint(),\n",
    "            MemoryMonitorCallback(threshold_gb=20),\n",
    "            EarlyStoppingCallback(early_stopping_patience=3)\n",
    "        ]\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_dataset[\"train\"],\n",
    "            eval_dataset=processed_dataset[\"valid\"],\n",
    "            compute_metrics=compute_metrics(processor),\n",
    "            data_collator=WhisperDataCollator(processor),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # 학습률 스케줄러 설정\n",
    "        num_training_steps = training_args.max_steps\n",
    "        num_warmup_steps = training_args.warmup_steps\n",
    "        \n",
    "        # optimizer가 None이 아닌지 확인\n",
    "        if trainer.optimizer is None:\n",
    "            print(\"Warning: Optimizer is None, initializing default optimizer...\")\n",
    "            trainer.create_optimizer()\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            trainer.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        trainer.lr_scheduler = scheduler\n",
    "        \n",
    "        # 최종 확인\n",
    "        print(\"\\n최종 모델 상태:\")\n",
    "        print(\"학습 모드:\", model.training)\n",
    "        print(\"학습 가능한 파라미터 수:\", sum(p.requires_grad for p in model.parameters()))\n",
    "        print(\"Optimizer:\", type(trainer.optimizer).__name__)\n",
    "        print(\"Learning rate:\", trainer.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if os.path.exists(output_dir):\n",
    "            checkpoints = [d for d in os.listdir(output_dir) \n",
    "                          if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, d))]\n",
    "            if checkpoints:\n",
    "                numeric_checkpoints = [d for d in checkpoints if d.replace(\"checkpoint-\", \"\").isdigit()]\n",
    "                if numeric_checkpoints:\n",
    "                    latest_checkpoint = sorted(numeric_checkpoints, \n",
    "                                              key=lambda x: int(x.replace(\"checkpoint-\", \"\")))[-1]\n",
    "                    print(f\"체크포인트에서 재개: {latest_checkpoint}\")\n",
    "                    trainer.train(resume_from_checkpoint=os.path.join(output_dir, latest_checkpoint))\n",
    "                else:\n",
    "                    print(\"자동 감지 가능한 체크포인트가 없어 처음부터 시작합니다.\")\n",
    "                    trainer.train()\n",
    "            else:\n",
    "                print(\"체크포인트가 없어 처음부터 시작합니다.\")\n",
    "                trainer.train()\n",
    "        else:\n",
    "            print(\"출력 디렉토리가 없어 처음부터 시작합니다.\")\n",
    "            trainer.train()\n",
    "        \n",
    "        final_dir = os.path.join(output_dir, \"final_model\")\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(final_dir, safe_serialization=True)\n",
    "        processor.save_pretrained(final_dir)\n",
    "        print(f\"모델 저장 완료: {final_dir}\")\n",
    "\n",
    "        validate_model(final_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_logger.log(f\"Critical Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# 메모리 최적화 및 훈련 시작\n",
    "def optimize_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"메모리 최적화 완료\")\n",
    "\n",
    "class WhisperPEFTModel(PeftModelForSeq2SeqLM):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 배치 크기 확인\n",
    "        batch_size = input_features.size(0)\n",
    "        \n",
    "        # 모든 입력의 배치 크기 확인 및 조정\n",
    "        if labels is not None:\n",
    "            if labels.size(0) != batch_size:\n",
    "                print(f\"Warning: Labels batch size mismatch. Expected {batch_size}, got {labels.size(0)}\")\n",
    "                if labels.size(0) > batch_size:\n",
    "                    labels = labels[:batch_size]\n",
    "                else:\n",
    "                    padding = torch.full((batch_size - labels.size(0), labels.size(1)), \n",
    "                                      self.processor.tokenizer.pad_token_id,\n",
    "                                      dtype=labels.dtype,\n",
    "                                      device=labels.device)\n",
    "                    labels = torch.cat([labels, padding], dim=0)\n",
    "        \n",
    "        if decoder_input_ids is not None:\n",
    "            if decoder_input_ids.size(0) != batch_size:\n",
    "                print(f\"Warning: Decoder input IDs batch size mismatch. Expected {batch_size}, got {decoder_input_ids.size(0)}\")\n",
    "                if decoder_input_ids.size(0) > batch_size:\n",
    "                    decoder_input_ids = decoder_input_ids[:batch_size]\n",
    "                else:\n",
    "                    padding = torch.full((batch_size - decoder_input_ids.size(0), decoder_input_ids.size(1)),\n",
    "                                      self.processor.tokenizer.pad_token_id,\n",
    "                                      dtype=decoder_input_ids.dtype,\n",
    "                                      device=decoder_input_ids.device)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, padding], dim=0)\n",
    "        \n",
    "        # PEFT 모델의 forward 메서드 호출\n",
    "        with self._enable_peft_forward_hooks(**kwargs):\n",
    "            kwargs = {k: v for k, v in kwargs.items() \n",
    "                     if k not in self.special_peft_forward_args \n",
    "                     and k not in [\"forced_decoder_ids\", \"use_cache\"]}\n",
    "            \n",
    "            outputs = self.base_model(\n",
    "                input_features=input_features,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                labels=labels,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "            \n",
    "            # 손실 계산 전 배치 크기 확인 및 조정\n",
    "            if hasattr(outputs, 'logits') and labels is not None:\n",
    "                logits = outputs.logits\n",
    "                if logits.size(0) != labels.size(0):\n",
    "                    print(f\"Warning: Logits batch size ({logits.size(0)}) != Labels batch size ({labels.size(0)})\")\n",
    "                    min_size = min(logits.size(0), labels.size(0))\n",
    "                    logits = logits[:min_size]\n",
    "                    labels = labels[:min_size]\n",
    "                    outputs.logits = logits\n",
    "                    \n",
    "                    # 손실 재계산\n",
    "                    if hasattr(outputs, 'loss'):\n",
    "                        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.processor.tokenizer.pad_token_id)\n",
    "                        outputs.loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            return outputs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimize_memory()\n",
    "    train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd028bf1-33f4-4723-a706-c67bf46ef15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
